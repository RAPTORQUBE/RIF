\relax 
\citation{bergman2008exascale}
\citation{ortega2013combining}
\citation{team2000r}
\citation{o2013artificial}
\@writefile{toc}{\contentsline {section}{\numberline {1}Research Question}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Can a Rapid Information Factory using agile and lean six sigma manufactory principles to solve the issues generated by effective and efficient exascale heterogeneous computing of a quintillion bytes data lake into a value-add deep learning knowledge source?}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {2}Rapid Information Factory (RIF)}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Rapid Information Factory Framework (RIFF)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The rapid information factory framework is a methodology, the result of research since 2008 , designed to guide a exascale \cite  {bergman2008exascale} heterogeneous computing cluster to process a exabyte data lake. The framework will processes a quintillion calculations per second against quintillion bytes of disk storage. The framework generates a series of virtual factories that together process the data lake using enhanced custom designed parallel processes.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Functional Layer}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {High-Level View}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The high-level view of the Homogeneous Ontology for Recursive Uniform Schema (HORUS) shows the users the current status of the rapid information factory. This is achieve by visualisation of the rapid information factory via a Rstudio Shiny \cite  {ortega2013combining} and R \cite  {team2000r} based web site. The complete state of the factories are online in a graph database.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Synaptic Assimilator (SA)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The synaptic assimilator (SA) is an artificial intelligence \cite  {o2013artificial} engine that performs the processes assigned to the system to the most effective and efficient method.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The artificial intelligence uses machine learning as an investigation and testing method to improve and select the correct combination of processing artifacts to achieve the efficient and effective outcome of each factory process.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The engine is taught data science and performance improvement processes to enables it to manage the factories to process the data sources it is supplied with for processing.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Exascale Data Lake}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The exascale data lake is a data source that exceeds a quintillion bytes. The data source holds structured, semi-structured and unstructured data. The synaptic assimilator converts it into a base deep learning data source by applying the appropriate functions and data processing patterns.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Persistent Recursive Information Schema Manipulator (PRISM)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The persistent recursive information schema manipulator is the central control framework for each data procesing flow through the system using a bulk synchronous parallel (BSP) abstract computer as a bridging model for rapid information factory's parallel algorithms, that are pre-defined and tested by each factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {RAPTOR Supersteps}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The RAPTOR framework is the basis for the six supersteps of the bulk synchronous parallel (BSP) based process. The framework uses fundamental building blocks like pipeline, farm and loopback to formulate more complex structures to handle the data requirements within each of the six supersteps.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The six supersteps are:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Retrieve Superstep}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The retrieve superstep is responsible for data retrieval from other data sources into the data lake. \textit  {(See Retrieve Superstep for more details)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Assess Superstep}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The assess superstep is responsible for the data validation in the active factory. Data Quality is determined and also improved where possible in this super step.\textit  {(See Assess Superstep for more details)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Process Superstep}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The process superstep is responsible for the processing of the dark data in the data lake into a structured data vault. The data vault keeps full record of the different phases that the data is process over time.\textit  {(See Process Superstep for more details)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Transform Superstep}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The transform superstep is responsible for transforming the data vault into an enterprise data warehouse. This superstep tranforms the data into knowledge by adding dimensionality and insight to the data vault.\textit  {(See Transform Superstep for more details)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Organise Superstep}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The organise superstep is responsible for organising the data sets together for each business grouping from the data warehose into data marts.\textit  {(See Organise Superstep for more details)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Report Superstep}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The report superstop is responsible to perform the required reporting and analytic requirements.\textit  {(See Report Superstep for more details)}}{\thepage }}
\citation{stone2010opencl}
\citation{malcolm2012arrayfire}
\citation{hintjens2011omq}
\citation{tanase2014highly}
\citation{mishra2014titan}
\citation{aldinucci2011accelerating}
\citation{feld2000lean}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Operational Management Layer}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The operational mangement layer is responsible to handle all the arctifacts required by the factory to perform the required processing requirements. It also bounds the factory to only be able to perform functions the layer already manages.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Autonomous Node Transport (ANT) Definitions}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The autonomous node transport definitions are the set of cloud instances or physical servers configurations supporting the processing capability of the factory. The HORUS schema keeps a series of characteristics required by the factory to decide which autonomous node transport to use for which requirement. The nodes will be changed in the future as characteristics for new nodes are discovered and loaded into the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The nodes are heterogeneous computing enabled and support combinations of heterogeneous processors that covers the range from high-end servers and high-performance computing machines to low-power embedded devices like mobile phones and tablets.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Autonomous Node Transport Management}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The autonomous node transport management oversees the complete process of running the heterogeneous computing systems. The combinations and work flow of the nodes is pre-defined in this section.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Monitoring}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Monitoring handles the monitoring tasks in the system. The monitoring covers all aspects of the factory's performance.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Persistent Uniform Protocol Agreement (PUPA) Definitions}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The persistent uniform protocol agreement definitions are the collection of the algorithmic skeletons within the system.The PUPAs are programs generate using existing frameworks like OpenCL \cite  {stone2010opencl}, ArrayFire \cite  {malcolm2012arrayfire}, Spark \cite  {hintjens2011omq}, Titan graph database \cite  {tanase2014highly} \cite  {mishra2014titan}, FastFlow Framework \cite  {aldinucci2011accelerating}. This is the main area of research for the team. The enhancement and creation of new parallel patterns will improve the capasity of the synaptic assimilator to build new factories and process the data into knowledge.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Persistent Uniform Protocol Agreement (PUPA) Management}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The persistent uniform protocol agreement management oversees the complete collection. The process ensures that all uniform protocol agreements are manange to achieve the required end goal of the factory to effectively and efficently process data into knowledge.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Alerting}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The alerts are manage from this singular point in the system. The alerting process interacts with the communication process to ensure the appropriate response is generated for the alerts.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Parameters}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The parameters are stored in this singular place in the system. Managing the parameters in a single location enhance sthe factory to adapt to changing requirements in a rapid time.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Scheduling}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The scheduling handles the schedules from this singular point in the system.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Communication}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Communication handles the communication into and from the system from this singular point.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Audit, Balance and Control Layer}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Work Cells}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The work cells \cite  {feld2000lean} is the fundamental building block of the processing system.}{\thepage }}
\citation{roman1985taxonomy}
\@writefile{toc}{\contentsline {paragraph}{The work cell is executing as an actor model (a mathematical model of concurrent computation) that use "actors" as the universal primitives of concurrent computation: in response to a message it receives, an actor can make local decisions, create more actors, send more messages, and determine how to respond to message received.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Execution Statistics}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The execution statistics is the fundamental performance recording system for the factory in the solution.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Remote Yoke}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Yoke or Poka-yoke is fundamental process of "mistake-proofing". The Remote York is the rapid information factory's fundamental monitoring interface between the different work cells. }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Rejections and Error Handling}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The rejections and error handling in the rapid information factory handles the rejections and error handling within the system.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Balancing and Control}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The balance and control mechanisms are the execute from the singular section.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Codes Management}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The code management is the single section that holds the standard codes used in the system.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Standard codes includes ISO codes, pre-agreed names and known lists of items for the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Example of the ISO standards used are:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The use of standard codes enable the effective deep data mining prescribed as output of the factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Business Layer}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Functional Requirements}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A functional requirement defines a function of a system and its components. A function is described as a set of inputs, the behavior, and outputs. \cite  {roman1985taxonomy}. The set of requirements together as a unit describes the factory processing rules.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Non-functional Requirements}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A non-functional requirement is a requirement that specifies criteria that can be used to test the operation of a factory, rather than specific behaviors of the process within the factory.The set of requirements together as a unit describes the factory verification rules.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The following are types of non-functional requirements}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Availability of factory as a factor of its reliability.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Certification of the factory can be achieved for several different ISO standards.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Compliance is achieved against a minimum set of criteria for the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Short response time for a spesific PUPA to complete.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{High throughput in the factory}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Low utilization of computing resources in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Requirement for timely response from the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The horizontal and vertical scalability is the ratio the factory can expand its resources for processing.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Utility Layer}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The utility layer stores processing structures across the factory for general or common requirements.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Maintenance Utilities}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The maintenance untilities are procesing structures that perform work for the factory to maintain}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Data Utilities}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Spesific Utilities}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Autonomous Logical Agreement Transport Executor (ALATE)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The autonomous logical agreement transport executor is a special utility that builds a fundamental metadata view of the data source it is processing and configures a fundamental set of PUPA that will form the fundamental factory for the data source.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Rapid Artifical Intelligence Data Extract Routine (RAIDER)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The rapid artifical intelligence data extract routine is a special utility that builds process PUPA for Retrieve of data via NEST PUPA.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Rapid Execute Artificial Protocol Engine for Routine (REAPER)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The rapid execute artificial protocol engine for routine is a special utility that performs a stand alone execution of any pre-approved work cell.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Sequencetial Converter into Ontology for Uniform Transport (SCOUT)}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The sequencetial converter into ontology for uniform transport is a special utility that builds NEST PUPA for a data source.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The SCOUT connects to the external data and discovers the metadata required to connect to the data source.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The output is a NEST script using HORUS scripts to be used by the factory to connect to the spesific data source.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Functional Layer}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Retrieve Superstep}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The retrieve superstep uses a series of work cells with an assembly format that is made up out of four components:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it complies to its assigned task in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Input PUPA or Input NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Assess Superstep}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The assess superstep uses a series of work cells with an assembly format that is made up out of four components:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Process Superstep}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The process superstep uses a series of work cells with an assembly format that is made up out of four components:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Transform Superstep}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The transform superstep uses a series of work cells with an assembly format that is made up out of four components:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Organise Superstep}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The organise superstep uses a series of work cells with an assembly format that is made up out of four components:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6}Report Superstep}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The report superstep uses a series of work cells with an assembly format that is made up out of four components:}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Work Cells}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The remote work cells is the fundamental processing container of the rapid information factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Monitor Work Cell}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The monitor work cell consists of a persistent recursive information schema manipulator plus a remote assessment yoke for each processing work cell the spesific BSP flow requires in the rapid information factory}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Processing Work Cell}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The processing work cell is a combination of a remote assessment yoke, an input persistent uniform protocol agreement, an autonomous node transport and an output persistent uniform protocol agreement. The remote assessment yoke communicates to the remote assessment yoke attached to the monitor work cell. The input persistent uniform protocol agreement holds the instructions to enable the work cell to import the data into the work cell. The output persistent uniform protocol agreement holds the instructions to enable the work cell to export the data from the work cell.The autonomous node transport supplies the processing power to execute the PUPA and the yoke instructions.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Measure Work Cell}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The measure work cell consists of an autonomous node transport that supplies the processing power and a measure agreement precision that supplies the tests to determine if the processing was successful.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Rapid Information Factory Data Sources}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Retrieve Data Sources}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The retrieve data sources are external data source that requires a spesial type of persistent uniform protocol agreement called a node extractor and schema transformer that supplies the data processing instructions to transform the extraernal data into HORUS compliant data structures. The additional data workspace supplies preloaded data to assist the retrieve superstep to load the data from the external data source to create the retrieve data workspace that is the main storage structure in HORUS any retrieve data loads.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Assess Data Sources}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The assess data sources are a read only input from the retrieve data workspace, a reference data workspace that is a read only data source for supplying reference data for the assess procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the assess data. The assess data workspace is the main storage structure for HORUS data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Process Data Sources}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The process data sources are a read only input from the assess data workspace, a reference data workspace that is a read only data source for supplying reference data for the process procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the process data. The process data workspace is the main storage structure for HORUS data processed into a data vault containing hubs, links and satellites. }{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Data Vault}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Data Vault architecture offers a unique solution to data integration in the rapid information factory. The Data Vault is a detail oriented, historical tracking and uniquely linked set of normalised tables that support one or more functional areas of the factory that stores perfeactly as data island on top of the data lake structure to process unstructured and semi-structured data into structured data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Benefits of Data Vault Modeling}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Transform Data Sources}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The transform data sources are a read only input from the process data workspace, a reference data workspace that is a read only data source for supplying reference data for the transform procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the transform data. The transform data workspace is the main storage structure for HORUS data warehouse structure that supports any analytic inquiries.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.6}Organise Data Sources}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The organise data sources are read only input from the Tranform data workspace, the organise data workspace to handle any organise data manipulation, the rapid information framework for datamarts, the rapid information framework for analytics and the rapid information framework for cubes that is the main storage structures for the factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7}Report Data Sources}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The report data sources are read only input from the rapid information framework for datamarts, the rapid information framework for analytics and the rapid information framework for cubes. The role based access contol security process enforces any role based security access to the data sources. The rapid information framework for visualistion handles the factory's visualisation requirements. The rapid information framework for exports are the export methord for the rapid information factory and formats the HORUS compliant data structures into external data formats via a persistent uniform protocol agreement.}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {3}Rapid Test Framework}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Unit testing}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Static Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{YOKE Unit Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The YOKE unit testing enables the rapid information factory to test all the YOKE structures individually.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Solution Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The solution testing performance the testing of the solution.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Solution Testing Plan}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The solution testing plan is the process description of how to test the solution as a complete factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Link Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Generate Link Test Data}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Prepare data for each Link Test to match the spesific measure agreement precision instructions.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Singular Link Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the Link Test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Parallel Link Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the Link Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}System Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Generate System Test Data}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Prepare data for each System Test to match the spesific measure agreement precision instructions.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Singular System Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the System Test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Parallel System Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the System Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Performance Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Generate Performance Test Data}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Prepare data for each Performance Test to match the spesific measure agreement precision instructions.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Singular Performance Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the Link Performance instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Parallel Performance Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the Performance Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Solution Completion Report}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The solution completion report is the combined data for the solution testing.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Acceptance Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The acceptance testing performance the testing of the solution.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Acceptance Testing Plan}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The acceptance testing plan is the process description of how to test the solution for acceptance by the users.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Acceptance Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Generate Acceptance Test Data}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Prepare data for each acceptance test to match the spesific measure agreement precision instructions.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Singular Acceptance Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the acceptance test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Parallel Acceptance Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the acceptance test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Exploratory Parallel Testing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Generate Exploratory Parallel Test Data}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Prepare data for each exploratory parallel test to match the spesific measure agreement precision instructions.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Singular Exploratory Parallel Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the exploratory parallel test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute Parallel Exploratory Parallel Test}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Execute the exploratory parallel test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Acceptance Completion Report}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The acceptance completion report is the combined data for the solution testing.}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}Process Layer - Data Vault}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A data vault consists of three types of data structures:}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Time-People-Object-Location-Event}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Time (Hub)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The time hub contains at the following fields:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Time Details (Satellite)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The time satellite contains at the following fields:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}People (Hub)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The people hub contains at the following fields:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}People Details (Satellite)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{??????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5}Object (Hub)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The object hub contains at the following fields:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.6}Object Details (Satellite)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7}Location (Hub)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The ISO 6709 Standard representation of geographic point location by coordinates.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The location hub contains at the following fields:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.8}Location Details (Satellite)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.9}Event (Hub)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The event hub contains at the following fields:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.10}Event Details (Satellite)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.11}Time People (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.12}Time Object (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.13}Time Location (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.14}Time Event (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.15}People Object (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.16}People Location (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.17}People Event (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.18}People People (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.19}Object Location (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.20}Object Event (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.21}Object object (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.22}Location Event (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.23}Location location (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.24}Event Event (Link)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{????}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {5}Transform Layer - Sun Models}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The transform layer converts the data into business aligned knowledge structures that the business can formulate queries to answer business questions.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dimensions}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The dimension is the information component of the business structure.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Type 0 Dimension}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Type 0 method is passive as it only insert but never updates.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Type 1 Dimension}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Type 1 method is active as it overwrites old with new data but keeps no track of historical data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Type 2 Dimension}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Type 2 method is active as it tracks historical data by creating multiple records for a given natural key and keeps track of period the values was valid.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Outrigger Dimension}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The outrigger dimension contain a reference to another dimension table to enable linking the two dimensions attributes.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}Bridge Dimension}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The bridge dimension is mapping solution that resolves many-to-many relationship within the dimension schema by converting it to a one-to-many and a many-to-one relationship.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.6}Mini Dimension}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The mini dimension creates a new structure that moves dimensional attributes that are rapidly changing into a miniâ€“dimension to split it from non-rapid changing dimensional attributes.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Facts}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A fact table consists of the measurements, metrics for business process.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Measures}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Measures are facts that store measurements and metrics that enable the apllication of mathematic process on the values.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Factless}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Factless facts are spesial structures that only consist of keys that links to the relavent dimensions to the facts.}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {6}Schedule Framework}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Cycle Time}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Cycle time describes the time use to complete a specific task from start to finish.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The time is calculated as:}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Value Stream}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Value stream mapping is the lean-management method for analysing the current state and designing a improved state for the series of activities.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Value Added Procesing Time}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The value added processing time is the time the factory process the data to add value.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Non Value Added Procesing Time}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The non value added processing time is the time the factory wastes while process the data.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Production Lead Time}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Production Lead Time is the total time (Value Added Procesing and Non Value Added Procesing) use to execute the data through an entire value stream.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Schedule Backlog}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The Schedule Backlog is all the required processing Persistent Uniform Protocol Agreements (PUPA) prioritised, ordered list, sorted by business value and risk. It contains the tasks to accomplish the RAPTOR flow. The Schedule Backlog often contains user stories covering functional requirements, non-functional requirements for the RAPTOR flow.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}INVEST RAPTOR flow}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Independent}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{RAPTOR flow must be independent. The PUPA must ensure that it does not interfere with other PUPAs.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Negotiable}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A RAPTOR flow always negotiable. It is not an explicit contract for functionality of the PUPA. The details will be co-created by the customer and programmer during development. The improvement process will ten ensure the output is delivered with maximum efficiency.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Valuable}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A RAPTOR flow must be valuable to business. No non-value add PUPA will execute in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Estimable}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A RAPTOR flow must be estimated. Any PUPA must have a estimate for execution.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Small}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{RAPTOR flow must be as small as possible. Each PUPA should only perform singular functions.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Testable}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A RAPTOR flow must be testable. Each PUPA should be testable for a known input to a know output.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}SMART Tasks}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Specific}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A RAPTOR flow task needs to be specific and tasks to add up to the full RAPTOR flow.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Measurable}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The key measure is testability for successful end-to-end results.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Achievable}}{\thepage }}
\citation{daniel1997scheduling}
\citation{daniel1997scheduling}
\@writefile{toc}{\contentsline {paragraph}{The task should be achievable. Each PUPA must hold a achievable function.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Relevant}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Every PUPA/task should be relevant and contributing to the RAPTOR flow in the factory. Stories are divided into tasks for the achievement of RAPTOR flow.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Time-boxed}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A PUPA/task should be time-boxed: limited to a specific duration.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}\textbf  {Active Process Backlog}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The active process backlog consists of the committed process tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows. The backlog uses a Drum â€“ Buffer â€“ Rope (DBR) scheduling methodology \cite  {daniel1997scheduling} that monitors the workcells for progress and then commit the next PUPA into the process.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}\textbf  {Active Process Work Cells}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The active process work cells is the combination of processing structures that is currently active in the rapid information factory. The quantity is determined by the available and required parallel processing units active in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.1}\textbf  {Process Set-up Time}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The process set-up time is the time it takes the factory to construct the work cell and bring it online ready for processing data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.2}\textbf  {Process Run Time}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The process run time is the time the work cell uses to process the data against the spesific input and output PUPAs' algoritmes.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.3}\textbf  {Process Reset Time}}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The process reset time is the time it takes the factory to reset the spesific work cell ready for the next work cell.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9}Verify Backlog}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The verify backlog consists of the committed process verify tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.10}Active Verify Backlog}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The active verify backlog consists of the committed verify tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.The backlog uses a Drum â€“ Buffer â€“ Rope (DBR) scheduling methodology \cite  {daniel1997scheduling} that monitors the workcells for progress and then commit the next PUPA into the process.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11}Active Verify Work Cells}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The active verify work cells is the combination of processing verify structures that is currently active in the rapid information factory. The quantity is determined by the available and required parallel processing units active in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.11.1}Verify Set-up Time}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The verify set-up time is the time it takes the factory to construct the work cell and bring it online ready for processing verification data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.11.2}Verify Run Time}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The verify run time is the time the work cell uses to verify the data against the spesific MAP's algoritmes.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.11.3}Verify Reset Time}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The verify reset time is the time it takes the factory to reset the spesific work cell ready for the next work cell.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12}Information Process Log}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The information process log consists of the completed process and verification tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {7}Improvement Processes}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}The 8 Wastes}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Defects.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Defects are mistakes that require extra time, resources and money to reprocess.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The defects are the result of:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Overproduction.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Overproduction is when too many of a spesific deliverable is delivered.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The factory would cause overproduction if too many ANTs is prepared for running PUPAs:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.3}Waiting}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Actual downtime that occurs whenever processing has to stop for an unplanned/planned reason.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Causes of waiting can also include:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.4}Non-Utilised Talent}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Non-Utilised Talent is the poor utilization of available talents, ideas, abilities and skill sets.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.5}Transportation}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Transportation is the unnecessary moving data around within the factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.6}Inventory}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Lean are based on the practice of Just-In-Time production of data in the factory.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Excess inventory is caused by:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.7}Motion}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Excess motion is to move around too much and then causes the factory slow down significantly.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Causes of excessive motion include:}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.8}Extra-processing}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Excess Processing is any unnecessary effort expended in order to complete a task: double-handling data, seeking permission during processing, unnecessary processing steps, unnecessary data useage, re-entering data, making too many copies of data.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Excess Processing arises from:}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Plan-Do-Act-Check Improvement Process}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Plan}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Establish the objectives and processes necessary to deliver results in accordance with the expected output (the target or goals). By establishing output expectations, the completeness and accuracy of the specifications is also a part of the targeted improvement. When possible start on a small scale to test possible effects.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Do}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Implement the plan, execute the process, execute the factory. Collect data for charting and analysis in the following "CHECK" and "ACT" steps.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Check}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Study the actual results (measured and collected in "DO" above) and compare against the expected results (targets or goals from the "PLAN") to ascertain any differences. Look for deviation in implementation from the plan and also look for the appropriateness and completeness of the plan to enable the execution, i.e., "Do". Charting data can make this much easier to see trends over several PDCA cycles and in order to convert the collected data into information. Information is what you need for the next step "ACT".}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.4}Act}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{If the CHECK shows that the PLAN that was implemented in DO is an improvement to the prior standard (baseline), then that becomes the new standard (baseline) for how the factory should ACT going forward (new standards are enACTed). If the CHECK shows that the PLAN that was implemented in DO is not an improvement, then the existing standard (baseline) will remain in place. In either case, if the CHECK showed something different than expected (whether better or worse), then there is more learning to be done and that will suggest potential future PDCA cycles. Note that some who teach PDCA assert that the ACT involves making adjustments or corrective actions. It is a repeating process that improves the factory.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Define-Measure-Analyse-Improve-Control Improvement Process}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Define}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The purpose of this step is to clearly articulate the business problem, goal, potential resources, project scope and high-level project timeline. This information is typically captured within project charter document.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Measure}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The purpose of this step is to objectively establish current baselines as the basis for improvement. This is a data collection step, the purpose of which is to establish process performance baselines.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Analyse}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The purpose of this step is to identify, validate and select root cause for elimination.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Improve}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The purpose of this step is to identify, test and implement a solution to the problem; in part or in whole.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.5}Control}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The purpose of this step is to sustain the gains. Monitor the improvements to ensure continued and sustainable success. Create a control plan.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Lean Six Sigma: 5S}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Sort}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The first step is to go through all equipment (ANTs and PUPAs) and data in the factory and determine what must be retained in the factory. Only essential PRISMs, ANTs and PUPA are allowed to remain. Archive any unwanted entities.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Set in Order}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{â€œA place for everything, and everything in its place.â€ }{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Work through the HORUS structures and ensure all the PRISMs, ANTs and PUPAs are in correct workflows. Use a directed acyclic graph (DAG) to ensure the workflow has a logical start and end. Simulate the process to run through without data to verify the factory before processing happens.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Shine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Monitor and maintain the processes the synaptic assimilator setup in HORUS, thoroughly clean everything remaining in the factories.Ensure effective and efficient execution of PRISMs, ANTs and PUPAs in the factories.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.4}Standardise}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Make factories consistent. All PRISMs and PUPAs should be identical so that any ANTs can immediately get initialise and productively execute the process if necessary.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.5}Systematise}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{This final step means to attach a schedule to use the 5S-ed factory flows.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Rapid Information Factory Cluster (RIFC)}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}3D Torus Network Framework}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Torus networks are use by top-performing supercompute because it ensures that node in the cluster can directly communicate with the other nodes. A 3D Torus Network enbles a minimum hop network.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}MapR Data Lake}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The rapid information factory cluster is a bulk synchronous parallel (BSP) engine. The cluster consisting of two hunderd thousand amazon cloud nodes (d2.8xlarge with thirty six processing cores, two hunderd forty four gigabyte memory and twenty four two thousand gigabyte hard disks) to support quintillion calculations per second against quintillion of disk storage.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.3}Titan Graph Data Lake}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The two hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.4}Cassandra Data Lake}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The five hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {8}Experiments}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Autonomous Node Transport - Build Test}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Amazon Web Services ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000001 - t2.micro - Amazon Cloud instance with 1 CPUs and 1 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000002 - t2.small - Amazon Cloud instance with 1 CPUs and 2 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000003 - t2.medium - Amazon Cloud instance with 2 CPUs and 4 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000004 - t2.large - Amazon Cloud instance with 2 CPUs and 8 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000005 - m4.large - Amazon Cloud instance with 2 CPUs and EBS-only Memory (GB) with 450 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000006 - m4.xlarge - Amazon Cloud instance with 4 CPUs and EBS-only Memory (GB) with 750 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000007 - m4.2xlarge - Amazon Cloud instance with 8 CPUs and EBS-only Memory (GB) with 1000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000008 - m4.4xlarge - Amazon Cloud instance with 16 CPUs and EBS-only Memory (GB) with 2000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000009 - m4.10xlarge - Amazon Cloud instance with 40 CPUs and EBS-only Memory (GB) with 4000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000010 - m3.medium - Amazon Cloud instance with 1 CPUs and 3.75 Memory (GB) with 1 x 4 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000011 - m3.large - Amazon Cloud instance with 2 CPUs and 7.5 Memory (GB) with 1 x 32 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000012 - m3.xlarge - Amazon Cloud instance with 4 CPUs and 15 Memory (GB) with 2 x 40 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000013 - m3.2xlarge - Amazon Cloud instance with 8 CPUs and 30 Memory (GB) with 2 x 80 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000014 - c4.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 500 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000015 - c4.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 750 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000016 - c4.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 1000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000017 - c4.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000018 - c4.8xlarge - Amazon Cloud instance with 36 CPUs and 60 Memory (GB) with 4000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000019 - c3.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 2 x 16 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000020 - c3.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 2 x 40 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000021 - c3.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 2 x 80 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000022 - c3.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2 x 160 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000023 - c3.8xlarge - Amazon Cloud instance with 32 CPUs and 60 Memory (GB) with 2 x 320 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000024 - r3.large - Amazon Cloud instance with 2 CPUs and 15.25 Memory (GB) with 1 x 32 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000025 - r3.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 80 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000026 - r3.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 1 x 160 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000027 - r3.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 1 x 320 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000028 - r3.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 2 x 320 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000029 - g2.2xlarge - Amazon Cloud instance with 1 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 8 CPUs and 15 Memory (GB) with 1 x 60 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000030 - g2.8xlarge - Amazon Cloud instance with 4 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 32 CPUs and 60 Memory (GB) with 2 x 120 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000031 - i2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000032 - i2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 2 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000033 - i2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 4 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000034 - i2.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 8 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000035 - d2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 3 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000036 - d2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 6 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000037 - d2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 12 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000038 - d2.8xlarge - Amazon Cloud instance with 36 CPUs and 244 Memory (GB) with 24 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Google Cloud Platform ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000001 - App Engine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000002 - Compute Engine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000003 - Container Engine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000004 - Cloud Storage}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000005 - Cloud Datastore}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000006 - Cloud SQL}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000007 - Cloud Big Table}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000008 - BigQuery}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000009 - Cloud Dataflow}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Cloud Pub/Sub}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Cloud Endpoints}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Translate API}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Prediction API}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}OpenStack ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000001 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000002 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000003 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000004 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.4}Physical Cluster ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000001 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000002 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000003 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000004 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Persistent Uniform Protocol Agreements - Build Test}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Amazon Web Services ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000001 - t2.micro - Amazon Cloud instance with 1 CPUs and 1 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000002 - t2.small - Amazon Cloud instance with 1 CPUs and 2 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000003 - t2.medium - Amazon Cloud instance with 2 CPUs and 4 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000004 - t2.large - Amazon Cloud instance with 2 CPUs and 8 Memory (GB)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000005 - m4.large - Amazon Cloud instance with 2 CPUs and EBS-only Memory (GB) with 450 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000006 - m4.xlarge - Amazon Cloud instance with 4 CPUs and EBS-only Memory (GB) with 750 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000007 - m4.2xlarge - Amazon Cloud instance with 8 CPUs and EBS-only Memory (GB) with 1000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000008 - m4.4xlarge - Amazon Cloud instance with 16 CPUs and EBS-only Memory (GB) with 2000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000009 - m4.10xlarge - Amazon Cloud instance with 40 CPUs and EBS-only Memory (GB) with 4000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000010 - m3.medium - Amazon Cloud instance with 1 CPUs and 3.75 Memory (GB) with 1 x 4 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000011 - m3.large - Amazon Cloud instance with 2 CPUs and 7.5 Memory (GB) with 1 x 32 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000012 - m3.xlarge - Amazon Cloud instance with 4 CPUs and 15 Memory (GB) with 2 x 40 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000013 - m3.2xlarge - Amazon Cloud instance with 8 CPUs and 30 Memory (GB) with 2 x 80 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000014 - c4.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 500 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000015 - c4.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 750 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000016 - c4.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 1000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000017 - c4.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000018 - c4.8xlarge - Amazon Cloud instance with 36 CPUs and 60 Memory (GB) with 4000 mbps Throughput}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000019 - c3.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 2 x 16 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000020 - c3.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 2 x 40 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000021 - c3.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 2 x 80 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000022 - c3.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2 x 160 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000023 - c3.8xlarge - Amazon Cloud instance with 32 CPUs and 60 Memory (GB) with 2 x 320 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000024 - r3.large - Amazon Cloud instance with 2 CPUs and 15.25 Memory (GB) with 1 x 32 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000025 - r3.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 80 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000026 - r3.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 1 x 160 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000027 - r3.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 1 x 320 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000028 - r3.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 2 x 320 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000029 - g2.2xlarge - Amazon Cloud instance with 1 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 8 CPUs and 15 Memory (GB) with 1 x 60 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000030 - g2.8xlarge - Amazon Cloud instance with 4 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 32 CPUs and 60 Memory (GB) with 2 x 120 GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000031 - i2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000032 - i2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 2 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000033 - i2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 4 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000034 - i2.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 8 x 800 SSD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000035 - d2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 3 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000036 - d2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 6 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000037 - d2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 12 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A001000038 - d2.8xlarge - Amazon Cloud instance with 36 CPUs and 244 Memory (GB) with 24 x 2000 HDD GB SSD}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Google Cloud Platform ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000001 - App Engine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000002 - Compute Engine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000003 - Container Engine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000004 - Cloud Storage}{\thepage }}
\citation{asanovic2006landscape}
\@writefile{toc}{\contentsline {paragraph}{ANT002000005 - Cloud Datastore}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000006 - Cloud SQL}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000007 - Cloud Big Table}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000008 - BigQuery}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000009 - Cloud Dataflow}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Cloud Pub/Sub}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Cloud Endpoints}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Translate API}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT002000010 - Prediction API}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}OpenStack ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000001 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000002 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000003 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT003000004 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.4}Physical Cluster ANTs}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000001 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000002 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000003 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ANT004000004 - ?????}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Persistent Uniform Protocol Agreement - Process (Fundamental)}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1}PUPA001000001 - Sort 1,000 keys assending}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.2}PUPA001000002 - Sort 1,000,000 keys assending}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.3}PUPA001000003 - Sort 1,000,000,000 keys assending}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.4}PUPA001000004 - Sort 1,000 keys across 10 nodes assending}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.5}PUPA001000005 - Sort 1,000,000 keys across 10 nodes assending}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.6}PUPA001000006 - Sort 1,000,000,000 keys 10 nodes assending}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Persistent Uniform Protocol Agreement - Process (Advance)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{ The factory should be able to handle the 13 Dwalves of High-Performance Processing \cite  {asanovic2006landscape}}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.1}PUPA002000001 - Dense Linear Algebra}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{The data structure is classic vector and matrix operations that are divided into Level 1 (vector/vector), Level 2 (matrix/vector), and Level 3 (matrix/matrix) operations. Data is normally a contiguous array and computations on elements, rows, columns, or matrix blocks to produce a output.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{General dense (all entries nonzero)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Banded (zero below/above some diagonal)}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Symmetric/Hermitian}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.2}PUPA002000002 - Sparse Linear Algebra}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Sparse matrix algorithms are use when input matrices contain a large number of zero entries. Gather-scatter is a type of memory addressing that often arises when addressing vectors in sparse linear algebra operations.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.3}PUPA002000003 - Spectral Methods}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Spectral methods are a class of techniques applying mathematics and scientific computing using Fast Fourier Transform.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Data is operated in the spectral domain, often transformed from either a temporal or spatial domain.}{\thepage }}
\citation{seo2010hama}
\citation{ching2013scaling}
\citation{wu2012kernel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.4}PUPA002000004 - N-Body Methods}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A n-body problem is the problem of predicting the individual motions of a group of celestial objects interacting with each other gravitationally.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Data is arranged in a regular multidimensional grid (most commonly 2D or 3D).}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.5}PUPA002000005 - Structured Grids}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A structured Grid is n-dimensional Euclidean space by congruent cells. Example: a 3-dimensional grid is of format m(x,y,z).}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.6}PUPA002000006 - Unstructured Grids}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A unstructured grid is a grid consisting of vertices and edges that describes a system of interconnected entities.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{An irregular mesh or grid, with each grid element being updated from neighboring grid elements.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.7}PUPA002000007 - Monte Carlo Simulations}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A problem solving technique used to approximate the probability of outcomes by executing multiple trials, called simulations, using randomised variables.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.8}PUPA002000008 - Combinational Logic}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A type of digital logic used by Boolean circuits, where the output is a pure function of the supplied input only.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Combinational logic computations are performing simple operations on very large amounts of data.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.9}PUPA002000009 - Graph Traversal}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Graph traversal is the problem of visiting all the nodes in a graph in a particular order.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Graph Traversal applications traverse a number of objects and examine characteristics of those objects to match spesific patterns.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.10}PUPA002000010 - Dynamic Programming}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Dynamic programming is a extremely powerful algorithmic paradigm solving a problem by identifying a collection of subproblems and solving them one by one, smallest first, using the results to small problems to solve larger ones, until the whole set of them is solved successfully.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.11}PUPA002000011 - Backtrack and Branch-and-Bound}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Branch and bound algorithms are effective for solving various search and global optimization problems. The goal in such problems is to search a very large space to find a globally optimal solution.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.12}PUPA002000012 - Graphical Models}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A graphical model or probabilistic graphical model (PGM) is a probabilistic model for a graph expressing the conditional dependence structure between random variables.}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Graphical models include Bayesian networks (also known as belief networks, probabilistic networks, causal network, and knowledge maps). Hidden Markov models and neural networks are also graphical models.}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.13}PUPA002000013 - Finite State Machine}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A finite-state machine (FSM) is a mathematical model of computation used to design computer programs and sequential logic circuits. It is modelled as an abstract machine that can be in one of a finite number of states.}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Persistent Uniform Protocol Agreement - Verify}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.1}?????????????????????????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{??????????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.2}?????????????????????????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{??????????}{\thepage }}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3}?????????????????????????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{??????????}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Apache Hama}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Apache Hama is a distributed computing framework based on Bulk Synchronous Parallel computing techniques for massive scientific computations e.g., matrix, graph and network algorithms.\cite  {seo2010hama}}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Apache Giraph}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Giraph is an iterative graph processing system built for high scalability.\cite  {ching2013scaling}}{\thepage }}
\citation{zhong2014medusa}
\citation{chang1997titan}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Weaver}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A fast and scalable graph store designed specifically for dynamically-changing graphs.\cite  {wu2012kernel}}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9}Medusa}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{A framework for graph processing using Graphics Processing Units (GPUs) on both shared memory and distributed clusters.\cite  {zhong2014medusa}}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10}Titan}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{Titan is a scalable graph database optimized for storing and querying graphs containing hundreds of billions of vertices and edges distributed across a multi-machine cluster. Titan is a transactional database that can support thousands of concurrent users executing complex graph traversals in real time. It supports ACID and eventual consistency and various big data storage back-ends.\cite  {chang1997titan}}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {9}Results}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{?????????????????????????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{?????????????????????????}{\thepage }}
\@writefile{toc}{\contentsline {paragraph}{?????????????????????????}{\thepage }}
\bibdata{andreasfrancoisvermeulen}
\bibcite{aldinucci2011accelerating}{1}
\bibcite{asanovic2006landscape}{2}
\bibcite{bergman2008exascale}{3}
\bibcite{chang1997titan}{4}
\bibcite{ching2013scaling}{5}
\bibcite{daniel1997scheduling}{6}
\bibcite{feld2000lean}{7}
\bibcite{hintjens2011omq}{8}
\bibcite{malcolm2012arrayfire}{9}
\bibcite{mishra2014titan}{10}
\bibcite{o2013artificial}{11}
\bibcite{ortega2013combining}{12}
\bibcite{roman1985taxonomy}{13}
\bibcite{seo2010hama}{14}
\bibcite{stone2010opencl}{15}
\bibcite{tanase2014highly}{16}
\bibcite{team2000r}{17}
\bibcite{wu2012kernel}{18}
\bibcite{zhong2014medusa}{19}
\bibstyle{acm}
\@writefile{toc}{\contentsline {paragraph}{?????????????????????????}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {10}References}{\thepage }}
