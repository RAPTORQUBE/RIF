\documentclass{acm_proc_article-sp}
\usepackage{graphicx}
\graphicspath{ {Image/} }
\DeclareGraphicsExtensions{.png,.jpg}
\begin{document}
\title{Parallel patterns against an exabyte data lake using exascale heterogeneous computing}
\subtitle{}
\numberofauthors{3} 
\author{
\alignauthor
Mr Andreas Vermeulen\\
\affaddr {\small University of St Andrews}\\
\affaddr {\small Saint Andrews, Fife KY16 9AJ}\\
\affaddr {\small University of Dundee}\\
\affaddr {\small Nethergate,Dundee DD1 4HN}\\
\email{{\small a.f.vermeulen@dundee.ac.uk}}
\alignauthor
Dr Vladimir Janjic\\
\affaddr {\small University of St Andrews}\\
\affaddr {\small Saint Andrews, Fife KY16 9AJ}\\
\email{{\small vj32@st-andrews.ac.uk}}
\alignauthor
Mr Andy Cobley\\
\affaddr {\small University of Dundee}\\
\affaddr {\small Nethergate, Dundee DD1 4HN}\\
\email{{\small acobley@computing.dundee.ac.uk}}
}
\date{1 June 2015}
\maketitle
\begin{abstract}
\textit{An enhancement of a research information factory using exascale heterogeneous computing and parallel knowledge-extraction patterns to generate deep learning source.}
\end{abstract}
\category{H.4}{Information Systems Applications}{Miscellaneous}
\terms{{Theory, Framework, Application, Research, Hardware}}
\keywords{{\textit{exabyte, exascale, knowledge-extraction, patterns, rapid information factory, RIF, RIFF, RIFC, heterogeneous computing, parallel patterns, mapr, cassandra, spark, opencl, R, fastflow, cuda, 3D torus network, deep learning, machine learning}}}
\section{Research Question}
\paragraph{Can a Rapid Information Factory using agile and lean six sigma principles to solve the effective and efficient exascale heterogeneous computing based processing of a million terabytes data lake into a value-add deep learning knowledge source?}
\newpage
\section{Rapid Information Factory}
\subsection{Rapid Information Factory Framework}
\paragraph{The rapid information factory framework is a methodology that guides a exascale \cite{bergman2008exascale} heterogeneous computing cluster to process a exabyte data lake. The framework processes a billion billion calculations per second against one million terabytes of disk storage. The framework generates a series of factories that together can process the data lake using custom designed parallel processes.}
\subsubsection{Functional Layer}
The functional layer is the layer that handles the functional processes within the cluster.
This layer is the bulk of the framework as it contains the main components of the factory process.
\paragraph{\textbf{High-Level View}}
\paragraph{The high-level view of the Homogeneous Ontology for Recursive Uniform Schema (HORUS) shows the users the current status of the Rapid Information Factory. This is achieve by visualisation of the rapid information factory via a Rstudio Shiny \cite{ortega2013combining} and R \cite{team2000r} based web site.}
\paragraph{\textbf{Synaptic Assimilator (SA)}}
\paragraph{The synaptic assimilator is an artificial intelligence \cite{o2013artificial} engine that performs the processes assigned to the system to the most effective and efficient method.}
\paragraph{The artificial intelligence uses machine learning as a investigation and testing method to improve and select the correct combination of processing artifacts to achieve the efficient and effective outcome.}
\paragraph{\textbf{Exascale Data Lake}}
\paragraph{The exascale data lake is a data source that is larger than thousand petabytes or one million terabytes or one billion gigabytes.}
\paragraph{\textbf{Persistent Recursive Information Schema Manipulator (PRISM)}}
\paragraph{The persistent recursive information schema manipulator is the central control framework for each data procesing flow through the system using a bulk synchronous parallel (BSP) abstract computer as a bridging model for rapid information factory's parallel algorithms that are pre-defined and tested by the factory.}
\paragraph{\textbf{RAPTOR Supersteps}}
\paragraph{The RAPTOR framework is the supersteps of the bulk synchronous parallel (BSP) based process. The framework uses basic building blocks like pipeline, farm and loopback to formulate more complex structures to handle the data requirements.}
\paragraph{The supersteps are:}
\paragraph{\textbf{Retrieve Superstep}}
\paragraph{The retrieve superstep is responsible for data retrieval from other data sources. \textit{(See Retrieve Superstep for more details)}}
\paragraph{\textbf{Assess Superstep}}
\paragraph{The assess superstep is responsible for the data validation in the factory.\textit{(See Assess Superstep for more details)}}
\paragraph{\textbf{Process Superstep}}
\paragraph{The process superstep is responsible for the processing of the data into a data vault that keeps full record of the different phases that the data is process over time.\textit{(See Process Superstep for more details)}}
\paragraph{\textbf{Transform Superstep}}
\paragraph{The transform superstep is responsible for transforming the data lake into a business formatted data warehouse.\textit{(See Transform Superstep for more details)}}
\paragraph{\textbf{Organise Superstep}}
\paragraph{The organise supersteo is responsible to orginise data sets together for each business group.\textit{(See Organise Superstep for more details)}}
\paragraph{\textbf{Report Superstep}}
\paragraph{The report superstop is responsible to perform the reporting requirements.\textit{(See Report Superstep for more details)}}
\subsubsection{Operational Management Layer}
\paragraph{\textbf{Autonomous Node Transport (ANT) Definitions}}
\paragraph{The autonomous node transport definitions are the set of cloud instances or physical servers that support the processing capability of the factory. The HORUS schema keeps a series of characteristics required by the factory to decide which ANT to use.}
\paragraph{The nodes are truly heterogeneous computing supporting combinations of processors that covers the range from high-end servers and high-performance computing machines all the way down to low-power embedded devices like mobile phones and tablets.}
\paragraph{\textbf{Autonomous Node Transport Management}}
\paragraph{The autonomous node transport management oversees the complete process of running the systems.}
\paragraph{\textbf{Monitoring}}
\paragraph{Monitoring handles the monitoring tasks in the system.}
\paragraph{\textbf{Persistent Uniform Protocol Agreement (PUPA) Definitions}}
\paragraph{The persistent uniform protocol agreement definitions are the collection of the algorithmic skeletons within the system.The PUPAs are programs generate using other frameworks like OpenCL \cite{stone2010opencl}, ArrayFire \cite{malcolm2012arrayfire}, Spark \cite{hintjens2011omq}, Titan graph database \cite{tanase2014highly} \cite{mishra2014titan}, FastFlow Framework \cite{aldinucci2011accelerating}.}
\paragraph{\textbf{Persistent Uniform Protocol Agreement (PUPA) Management}}
\paragraph{The persistent uniform protocol agreement management oversees the complete collection.}
\paragraph{\textbf{Alerting}}
\paragraph{The alerts are manage from this singular point in the system.}
\paragraph{\textbf{Parameters}}
\paragraph{The parameters are stored in this singular place in the system.}
\paragraph{\textbf{Scheduling}}
\paragraph{The scheduling handles the schedules from this singular point in the system.}
\paragraph{\textbf{Communication}}
\paragraph{Communication handles the communication into and from the system from this singular point.}
\subsubsection{Audit, Balance and Control Layer}
\paragraph{\textbf{Work Cells}}
\paragraph{The work cells \cite{feld2000lean} is the basic building block of the processing system.}
\paragraph{\textbf{Execution Statistics}}
\paragraph{The execution statistics is the basic performance recording system.}
\paragraph{\textbf{Remote Yoke}}
\paragraph{The Poka-yoke is term that means "mistake-proofing". The Remote York is the rapid information factory's basic monitoring interface between the different work cells. }
\paragraph{\textbf{Rejections and Error Handling}}
\paragraph{The rejections and error handling in the rapid information factory handles the rejections and error handling within the system.}
\paragraph{Balancing and Control}
\paragraph{The balance and control mechanisms are the execute from the singular section.}
\paragraph{Codes Management}
\paragraph{The code management is the single section that holds the standard codes used in the system.}
\paragraph{Stadard codes includes ISO codes, pre-agreed names and known lists of items for the factory.}
\paragraph{The use of standard codes enable the effective deep data mining prescribed as output of the factory.}
\subsubsection{Business Layer}
\paragraph{Functional Requirements}
\paragraph{A functional requirement defines a function of a system and its components. A function is described as a set of inputs, the behavior, and outputs. \cite{roman1985taxonomy}. The set of requirements together as a unit describes the factory processing rules.}
\paragraph{Non-functional Requirements}
\paragraph{A non-functional requirement is a requirement that specifies criteria that can be used to test the operation of a factory, rather than specific behaviors of the process within the factory.The set of requirements together as a unit describes the factory verification rules.}
\paragraph{The following are types of non-functional requirements}
\begin{itemize} 
\item Accessibility
\item Audit and control
\item Availability
\item Backup
\item Capacity (current and forecast)
\item Certification
\item Compliance
\item Configuration management
\item Dependency on other parties
\item Deployment
\item Documentation
\item Disaster recovery
\item Efficiency (resource consumption for given load)
\item Effectiveness (resulting performance in relation to effort)
\item Emotional factors (like fun or absorbing or has "Wow! Factor")
\item Environmental protection
\item Escrow
\item Exploitability
\item Extensibility (adding features, and carry-forward of customizations at next major version upgrade)
\item Failure management
\item Fault tolerance (e.g. Operational System Monitoring, Measuring, and Management)
\item Legal and licensing issues or patent-infringement-avoidability
\item Interoperability
\item Maintainability
\item Modifiability
\item Network topology
\item Open source
\item Operability
\item Performance / response time (performance engineering)
\item Platform compatibility
\item Price
\item Privacy
\item Portability
\item Quality (e.g. faults discovered, faults delivered, fault removal efficacy)
\item Recovery / recoverability (e.g. mean time to recovery - MTTR)
\item Reliability (e.g. mean time between failures - MTBF, or availability)
\item Reporting
\item Resilience
\item Resource constraints (processor speed, memory, disk space, network bandwidth)
\item Response time
\item Reusability
\item Robustness
\item Safety or Factor of safety
\item Scalability (horizontal, vertical)
\item Security
\item Software tools
\item Stability
\item Standards
\item Supportability
\item Testability
\item Usability by user community
\item User Friendliness
\end{itemize}
\subsubsection{Utility Layer}
\paragraph{The utility layer stores processing structures across the factory for general or common requirements.}
\paragraph{Maintenance Utilities}
\paragraph{The maintenance untilities are procesing structures that perform work for the factory to maintain}
\paragraph{Data Utilities}
\paragraph{Spesific Utilities}
\paragraph{\textbf{Autonomous Logical Agreement Transport Executor (ALATE)}}
\paragraph{The autonomous logical agreement transport executor is a special utility that ... }
\paragraph{\textbf{Rapid Artifical Intelligence Data Extract Routine (RAIDER)}}
\paragraph{The rapid artifical intelligence data extract routine is a special utility that ... }
\paragraph{\textbf{Rapid Execute Artificial Protocol Engine for Routine (REAPER)}}
\paragraph{The rapid execute artificial protocol engine for routine is a special utility that ... }
\paragraph{\textbf{Sequencetial Converter into Ontology for Uniform Transport (SCOUT)}}
\paragraph{The sequencetial converter into ontology for uniform transport is a special utility that ... }
\subsection{Functional Layer}
\subsubsection{Retrieve Superstep}
\paragraph{The retrieve superstep uses a series of work cells with an assembly format that is .......}
\subsubsection{Assess Superstep}
\paragraph{The assess superstep uses a series of work cells with an assembly format that is .......}
\subsubsection{Process Superstep}
\paragraph{The process superstep uses a series of work cells with an assembly format that is .......}
\subsubsection{Transform Superstep}
\paragraph{The transform superstep uses a series of work cells with an assembly format that is .......}
\subsubsection{Organise Superstep}
\paragraph{The organise superstep uses a series of work cells with an assembly format that is .......}
\subsubsection{Report Superstep}
\paragraph{The report superstep uses a series of work cells with an assembly format that is .......}
\subsection{Work Cells}
\paragraph{The remote work cells is the basic processing container of the rapid information factory.}
\subsubsection{Monitor Work Cell}
\paragraph{The monitor work cell consists of a persistent recursive information schema manipulator plus a remote assessment yoke for each processing work cell the spesific BSP flow requires in the rapid information factory}
\subsubsection{Processing Work Cell}
\paragraph{The processing work cell is a combination of a remote assessment yoke, an input persistent uniform protocol agreement, an autonomous node transport and an output persistent uniform protocol agreement. The remote assessment yoke communicates to the remote assessment yoke attached to the monitor work cell. The input persistent uniform protocol agreement holds the instructions to enable the work cell to import the data into the work cell. The output persistent uniform protocol agreement holds the instructions to enable the work cell to export the data from the work cell.The autonomous node transport supplies the processing power to execute the PUPA and the yoke instructions.}
\subsubsection{Measure Work Cell}
\paragraph{The measure work cell consists of an autonomous node transport that supplies the processing power and a measure agreement precision that supplies the tests to determine if the processing was successful.}
\newpage
\subsection{Rapid Information Factory Data Sources}
\subsubsection{Retrieve Data Sources}
\paragraph{The retrieve data sources are external data source that requires a spesial type of persistent uniform protocol agreement called a node extractor and schema transformer that supplies the data processing instructions to transform the extraernal data into HORUS compliant data structures. The additional data workspace supplies preloaded data to assist the retrieve superstep to load the data from the external data source to create the retrieve data workspace that is the main storage structure in HORUS any retrieve data loads.}
\subsubsection{Assess Data Sources}
\paragraph{The assess data sources are a read only input from the retrieve data workspace, a reference data workspace that is a read only data source for supplying reference data for the assess procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the assess data. The assess data workspace is the main storage structure for HORUS data.}
\subsubsection{Process Data Sources}
\paragraph{The process data sources are a read only input from the assess data workspace, a reference data workspace that is a read only data source for supplying reference data for the process procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the process data. The process data workspace is the main storage structure for HORUS data processed into a data vault containing hubs, links and satellites. }
\subsubsection{Data Vault}
\paragraph{The Data Vault architecture offers a unique solution to data integration in the rapid information factory. The Data Vault is a detail oriented, historical tracking and uniquely linked set of normalised tables that support one or more functional areas of the factory that stores perfeactly as data island on top of the data lake structure to process unstructured and semi-structured data into structured data.}
\paragraph{Benefits of Data Vault Modeling}
\begin{itemize}
\item Manage and Enforce Compliance to Sarbanes-Oxley, HIPPA, and BASIL II in factory
\item Spot business data issues that were bot visible before the processing
\item Rapidly reduce business cycle time for implementing changes
\item Merge new business units into the organisation rapidly
\item Rapid return-on-investment and Delivery of information to new star schemas
\item Consolidate disparate data stores.
\item Proper Master Data Management
\item Implement and Deploy service-oriented architecture (SOA)
\item Scale to exabytes of data
\item SEI CMM Level 5 compliant (Repeatable, consistent, redundant architecture)
\item Trace all data back to the source systems
\end{itemize}
\subsubsection{Transform Data Sources}
\paragraph{The transform data sources are a read only input from the process data workspace, a reference data workspace that is a read only data source for supplying reference data for the transform procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the transform data. The transform data workspace is the main storage structure for HORUS data warehouse structure that supports any analytic inquiries.}
\subsubsection{Organise Data Sources}
\paragraph{The organise data sources are read only input from the Tranform data workspace, the organise data workspace to handle any organise data manipulation, the rapid information framework for datamarts, the rapid information framework for analytics and the rapid information framework for cubes that is the main storage structures for the factory.}
\subsubsection{Report Data Sources}
\paragraph{The report data sources are read only input from the rapid information framework for datamarts, the rapid information framework for analytics and the rapid information framework for cubes. The role based access contol security process enforces any role based security access to the data sources. The rapid information framework for visualistion handles the factory's visualisation requirements. The rapid information framework for exports are the export methord for the rapid information factory and formats the HORUS compliant data structures into external data formats via a persistent uniform protocol agreement.}
\newpage
\section{Rapid Test Framework}
\subsection{Unit testing}
\subsubsection{Static Testing}
\paragraph{YOKE Unit Testing}
\paragraph{The YOKE unit testing enables the rapid information factory to test all the YOKE structures individually.}
\subsection{Solution Testing}
\paragraph{The solution testing performance the testing of the solution.}
\paragraph{Solution Testing Plan}
\paragraph{The solution testing plan is the process description of how to test the solution as a complete factory.}
\subsubsection{Link Testing}
\paragraph{Generate Link Test Data}
\paragraph{Prepare data for each Link Test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Link Test}
\paragraph{Execute the Link Test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Link Test}
\paragraph{Execute the Link Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\subsubsection{System Testing}
\paragraph{Generate System Test Data}
\paragraph{Prepare data for each System Test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular System Test}
\paragraph{Execute the System Test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel System Test}
\paragraph{Execute the System Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\subsubsection{Performance Testing}
\paragraph{Generate Performance Test Data}
\paragraph{Prepare data for each Performance Test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Performance Test}
\paragraph{Execute the Link Performance instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Performance Test}
\paragraph{Execute the Performance Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Solution Completion Report}
\paragraph{The solution completion report is the combined data for the solution testing.}
\subsection{Acceptance Testing}
\paragraph{The acceptance testing performance the testing of the solution.}
\paragraph{Acceptance Testing Plan}
\paragraph{The acceptance testing plan is the process description of how to test the solution for acceptance by the users.}
\subsubsection{Acceptance Testing}
\paragraph{Generate Acceptance Test Data}
\paragraph{Prepare data for each acceptance test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Acceptance Test}
\paragraph{Execute the acceptance test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Acceptance Test}
\paragraph{Execute the acceptance test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\subsubsection{Exploratory Parallel Testing}
\paragraph{Generate Exploratory Parallel Test Data}
\paragraph{Prepare data for each exploratory parallel test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Exploratory Parallel Test}
\paragraph{Execute the exploratory parallel test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Exploratory Parallel Test}
\paragraph{Execute the exploratory parallel test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Acceptance Completion Report}
\paragraph{The acceptance completion report is the combined data for the solution testing.}
\newpage
\section{Process Layer - Data Vault}
\subsection{Time-People-Object-Location-Event}
\subsubsection{Time (Hub)}
\subsubsection{Time Details (Satellite)}
\subsubsection{People (Hub)}
\subsubsection{People Details (Satellite)}
\subsubsection{Object (Hub)}
\subsubsection{Object Details (Satellite)}
\subsubsection{Location (Hub)}
\subsubsection{Location Details (Satellite)}
\subsubsection{Event (Hub)}
\subsubsection{Event Details (Satellite)}
\subsubsection{Time People (Link)}
\subsubsection{Time Object (Link)}
\subsubsection{Time Location (Link)}
\subsubsection{Time Event (Link)}
\subsubsection{People Object (Link)}
\subsubsection{People Location (Link)}
\subsubsection{People Event (Link)}
\subsubsection{People People (Link)}
\subsubsection{Object Location (Link)}
\subsubsection{Object Event (Link)}
\subsubsection{Object object (Link)}
\subsubsection{Location Event (Link)}
\subsubsection{Location location (Link)}
\subsubsection{Event Event (Link)}
\newpage
\section{Transform Layer - Sun Models}
\subsection{Dimensions}
\subsubsection{Type 0 Dimension}
The Type 0 method is passive as it only insert but never updates.
\subsubsection{Type 1 Dimension}
The Type 1 method is active as it overwrites old with new data but keeps no track of historical data.
\subsubsection{Type 2 Dimension}
The Type 2 method is active as it tracks historical data by creating multiple records for a given natural key and keeps track of period the values was valid.
\subsubsection{Outrigger Dimension}
The outrigger dimension is ....
\subsubsection{Bridge Dimension}
The bridge dimension is ...
\subsubsection{Mini Dimension}
The mini dimension is ...
\subsection{Facts}
Facts are ....
\subsubsection{Measures}
Measures are ...
\subsubsection{Factless}
Factless facts are ...
\newpage
\section{Schedule Framework}
\subsection{Cycle Time}
\paragraph{Cycle time describes the time use to complete a specific task from start to finish.}
\paragraph{The time is calculated as .....}
\subsection{Value Stream}
\paragraph{Value stream mapping is the lean-management method for analysing the current state and designing a improved state for the series of activities.}
\subsection{Value Added Procesing Time}
\paragraph{The value added processing time is the time the factory process the data to add value.}
\subsection{Non Value Added Procesing Time}
\paragraph{The non value added processing time is the time the factory wastes while process the data.}
\subsection{Production Lead Time}
\paragraph{Production Lead Time is the total time (Value Added Procesing and Non Value Added Procesing) use to execute the data through an entire value stream.}
\subsection{Schedule Backlog}
\paragraph{The Schedule Backlog is all the required processing Persistent Uniform Protocol Agreements (PUPA) prioritised, ordered list, sorted by business value and risk. It contains the tasks to accomplish the RAPTOR flow. The Schedule Backlog often contains user stories covering functional requirements, non-functional requirements for the RAPTOR flow.}
\subsubsection{INVEST RAPTOR flow}
\paragraph{Independent}
\paragraph{RAPTOR flow must be independent.}
\paragraph{Negotiable}
\paragraph{A RAPTOR flow always negotiable. It is not an explicit contract for features; rather, details will be co-created by the customer and programmer during development.}
\paragraph{Valuable}
\paragraph{A RAPTOR flow must be valuable to business.}
\paragraph{Estimable}
\paragraph{A RAPTOR flow must be estimated.}
\paragraph{Small}
\paragraph{RAPTOR flow must be as small as possible.}
\paragraph{Testable}
\paragraph{A RAPTOR flow must be testable.}
\subsubsection{SMART Tasks}
\paragraph{Specific}
\paragraph{A RAPTOR flow task needs to be specific and tasks to add up to the full RAPTOR flow.}
\paragraph{Measurable}
\paragraph{The key measure is testability}
\paragraph{Achievable}
\paragraph{The task should be achievable.}
\paragraph{Relevant}
\paragraph{Every task should be relevant and contributing to the RAPTOR flow in the factory. Stories are divided into tasks for the achievement of RAPTOR flow.}
\paragraph{Time-boxed}
\paragraph{A task should be time-boxed: limited to a specific duration.}
\subsection{Active Process Backlog}
\paragraph{The active process backlog consists of the committed process tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.}
\subsection{Active Process Work Cells}
\paragraph{The active process work cells is the combination of processing structures that is currently active in the rapid information factory. The quantity is determined by the available and required parallel processing units active in the factory.}
\subsubsection{Process Set-up Time}
\paragraph{The process set-up time is the time it takes the factory to construct the work cell and bring it online ready for processing data.}
\subsubsection{Process Run Time}
\paragraph{The process run time is the time the work cell uses to process the data against the spesific input and output PUPAs' algoritmes.}
\subsubsection{Process Reset Time}
\paragraph{The process reset time is the time it takes the factory to reset the spesific work cell ready for the next work cell.}
\subsection{Verify Backlog}
\paragraph{The verify backlog is .......}
\subsection{Active Verify Backlog}
\paragraph{The active process backlog consists of the committed verify tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.}
\subsection{Active Verify Work Cells}
\paragraph{The active verify work cells are .......}
\subsubsection{Verify Set-up Time}
\paragraph{The verify set-up time is .......}
\subsubsection{Verify Run Time}
\paragraph{The verify run time is .......}
\subsubsection{Verify Reset Time}
\paragraph{The verify reset time is .......}
\subsection{Information Process Log}
\paragraph{The information process log is .......}
\newpage
\section{Improvement Processes}
\subsection{The 8 Wastes}
\subsubsection{Defects}
\paragraph{Defects are mistakes that require extra time, resources and money to reprocess.}
\paragraph{The defects are the result of:}
\begin{itemize}
\item Poor quality control of processes
\item Poor repairs on servers
\item Poor documentation procedures
\item Lack of standards
\item Weak or missing processes
\item A misunderstanding of customer requirements
\item Poor inventory control
\item Poor design of processes
\item Undocumented design changes
\end{itemize}
\subsubsection{Overproduction}
\paragraph{Overproduction is when too many of a spesific deliverable is delivered.}
\paragraph{The factory would cause overproduction if too many ANT is prepared for running PUPAs:}
\begin{itemize}
\item Just-in-case production
\item Unclear customer requirements
\item Producing to a incorrect forecast
\item Long set-up times
\item Attempts to avoid long set-up times
\item Poorly applied automation 
\end{itemize}
\subsubsection{Waiting}
\paragraph{Actual downtime that occurs whenever processing has to stop for some reason.}
\paragraph{Causes of waiting can also include:}
\begin{itemize}
\item Mismatched production rates
\item Very long set-up times
\item Poor shop layout
\item Insufficient staffing
\item Work absences
\item Poor communications
\end{itemize}
\subsubsection{Non-Utilised Talent}
\paragraph{Non-Utilised Talent is the poor utilization of available talents, ideas, abilities and skill sets.}
\begin{itemize}
\item Lack of teamwork between staff members
\item Lack of training in operations of the factory
\item Poor communications between staff members and the factory
\item Management's refusal to include employees in problem-solving
\item Narrowly defined jobs and expectations for staff
\item Poor management in general of staff
\end{itemize}
\subsubsection{Transportation}
\paragraph{Transportation is the unnecessary moving data around within the factory.}
\begin{itemize}
\item Poor factory layout
\item Excessive or unnecessary handling of data
\item Misaligned process flow in the factory
\item Poorly-designed factory PUPAs
\item Unnecessary steps in factory processes
\end{itemize}
\subsubsection{Inventory}
\paragraph{Lean are based on the practice of Just-In-Time production of data in the factory.}
\paragraph{Excess inventory is caused by:}
\begin{itemize}
\item Overproduction
\item Poor layout
\item Mismatched production speeds
\item Unreliable suppliers
\item Long set-up times
\item Misunderstood customer needs
\end{itemize}
\subsubsection{Motion}
\paragraph{Excess motion is to move around too much and then causes the factory slow down significantly.}
\paragraph{Causes of excessive motion include:}
\begin{itemize}
\item Poor workstation/shop layout
\item Poor housekeeping
\item Shared tools and machines
\item Workstation congestion
\item Isolated operations
\item Lack of standards
\item Poor process design and controls
\end{itemize}
\subsubsection{Extra-processing}
\paragraph{Excess Processing is any unnecessary effort expended in order to complete a task: double-handling data, seeking permission during processing, unnecessary processing steps, unnecessary data useage, re-entering data, making too many copies of data.}
\paragraph{Excess Processing arises from:}
\begin{itemize}
\item Poor process control
\item Lack of standards
\item Poor communication
\item Overdesigned equipment
\item Misunderstanding of the customer's needs
\item Human error
\item Producing to forecast
\end{itemize}
\subsection{Plan-Do-Act-Check Improvement Process}
\subsubsection{Plan}
\paragraph{Establish the objectives and processes necessary to deliver results in accordance with the expected output (the target or goals). By establishing output expectations, the completeness and accuracy of the spec is also a part of the targeted improvement. When possible start on a small scale to test possible effects.}
\subsubsection{Do}
\paragraph{Implement the plan, execute the process, make the product. Collect data for charting and analysis in the following "CHECK" and "ACT" steps.}
\subsubsection{Check}
\paragraph{Study the actual results (measured and collected in "DO" above) and compare against the expected results (targets or goals from the "PLAN") to ascertain any differences. Look for deviation in implementation from the plan and also look for the appropriateness and completeness of the plan to enable the execution, i.e., "Do". Charting data can make this much easier to see trends over several PDCA cycles and in order to convert the collected data into information. Information is what you need for the next step "ACT".}
\subsubsection{Act}
\paragraph{If the CHECK shows that the PLAN that was implemented in DO is an improvement to the prior standard (baseline), then that becomes the new standard (baseline) for how the organization should ACT going forward (new standards are enACTed). If the CHECK shows that the PLAN that was implemented in DO is not an improvement, then the existing standard (baseline) will remain in place. In either case, if the CHECK showed something different than expected (whether better or worse), then there is some more learning to be done... and that will suggest potential future PDCA cycles. Note that some who teach PDCA assert that the ACT involves making adjustments or corrective actions... but generally it would be counter to PDCA thinking to propose and decide upon alternative changes without using a proper PLAN phase, or to make them the new standard (baseline) without going through DO and CHECK steps.}
\subsection{Define-Measure-Analyse-Improve-Control Improvement Process}
\subsubsection{Define}
\paragraph{The purpose of this step is to clearly articulate the business problem, goal, potential resources, project scope and high-level project timeline. This information is typically captured within project charter document.}
\subsubsection{Measure}
\paragraph{The purpose of this step is to objectively establish current baselines as the basis for improvement. This is a data collection step, the purpose of which is to establish process performance baselines.}
\subsubsection{Analyse}
\paragraph{The purpose of this step is to identify, validate and select root cause for elimination.}
\subsubsection{Improve}
\paragraph{The purpose of this step is to identify, test and implement a solution to the problem; in part or in whole.}
\subsubsection{Control}
\paragraph{The purpose of this step is to sustain the gains. Monitor the improvements to ensure continued and sustainable success. Create a control plan.}
\subsection{Lean Six Sigma: 5S}
\subsubsection{Sort}
\paragraph{The first step is to go through all equipment and materials and determine what must be retained at the worksite. Only essential tools, aids, equipment, and so on are allowed to remain.}
\subsubsection{Set in Order}
\paragraph{“A place for everything, and everything in its place.” }
\subsubsection{Shine}
\paragraph{To help maintain the order you’ve created, thoroughly clean everything remaining at the worksite.}
\subsubsection{Standardise}
\paragraph{Where possible, make worksites consistent. All workstations for a particular job should be identical so that someone from another worksite can immediately step in and productively run the process if necessary.}
\subsubsection{Systematise}
\paragraph{This final step means to put a schedule and system in place for maintaining and refreshing the 5S-ed worksite.}
\newpage
\subsection{Rapid Information Factory Cluster (RIFC)}
\subsubsection{3D Torus Network Framework}
\subsubsection{MapR Data Lake}
\paragraph{The rapid information factory cluster is a bulk synchronous parallel (BSP) engine. The cluster consisting of two hunderd thousand amazon cloud nodes (d2.8xlarge with thirty six processing cores, two hunderd forty four gigabyte memory and twenty four two thousand gigabyte hard disks) to support billion billion calculations per second against one million terabytes of disk storage.}
\paragraph{The hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}
\subsubsection{Titan Graph Data Lake}
\paragraph{The two hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}\subsubsection{Cassandra Data Lake}
\paragraph{The five hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}
\newpage
\section{Experiments}
\section{Results}
\newpage
\bibliography{andreasfrancoisvermeulen}
\bibliographystyle{acm}
\end{document}
