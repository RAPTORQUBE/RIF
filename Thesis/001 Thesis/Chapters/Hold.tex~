\subsection{Rapid Information Factory Framework (RIFF)}
\subsubsection{Business Layer}

\subsection{Functional Layer}
\subsubsection{Retrieve Superstep}
\paragraph{The retrieve superstep uses a series of work cells with an assembly format that is made up out of four components:}
\begin{itemize}
\item{Remote Monitor Yoke}
\item{Input PUPA or Input NEST PUPA}
\item{ANT}
\item{Output PUPA}
\end{itemize}
\paragraph{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it complies to its assigned task in the factory.}
\paragraph{The Input PUPA or Input NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}
\paragraph{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }
\paragraph{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}
\subsubsection{Assess Superstep}
\paragraph{The assess superstep uses a series of work cells with an assembly format that is made up out of four components:}
\begin{itemize}
\item{Remote Monitor Yoke}
\item{Input PUPA}
\item{ANT}
\item{Output PUPA}
\end{itemize}
\paragraph{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}
\paragraph{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}
\paragraph{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }
\paragraph{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}
\subsubsection{Process Superstep}
\paragraph{The process superstep uses a series of work cells with an assembly format that is made up out of four components:}
\begin{itemize}
\item{Remote Monitor Yoke}
\item{Input PUPA}
\item{ANT}
\item{Output PUPA}
\end{itemize}
\paragraph{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}
\paragraph{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}
\paragraph{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }
\paragraph{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}
\subsubsection{Transform Superstep}
\paragraph{The transform superstep uses a series of work cells with an assembly format that is made up out of four components:}
\begin{itemize}
\item{Remote Monitor Yoke}
\item{Input PUPA}
\item{ANT}
\item{Output PUPA}
\end{itemize}
\paragraph{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}
\paragraph{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}
\paragraph{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }
\paragraph{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}
\subsubsection{Organise Superstep}
\paragraph{The organise superstep uses a series of work cells with an assembly format that is made up out of four components:}
\begin{itemize}
\item{Remote Monitor Yoke}
\item{Input PUPA}
\item{ANT}
\item{Output PUPA}
\end{itemize}
\paragraph{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}
\paragraph{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}
\paragraph{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }
\paragraph{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}
\subsubsection{Report Superstep}
\paragraph{The report superstep uses a series of work cells with an assembly format that is made up out of four components:}
\begin{itemize}
\item{Remote Monitor Yoke}
\item{Input PUPA}
\item{ANT}
\item{Output PUPA}
\end{itemize}
\paragraph{The remote monitoring yoke connects the work cell to the PRISM that controls the spesific factory to enable the communication and control to the PRISM's remote motering yoke to ensure the process is monitored and that it comples it assigned task in the factory.}
\paragraph{The Input PUPA NEST PUPA describes the processsing rules and formats of the data source use as the input to the process. The factory translates the HORUS scripts to generate the processing logic to input the data.}
\paragraph{The ANT is the setup script in HORUS rules that builds a processing engine to process the data form the input PUPA into the processing rules of the output PUPA. }
\paragraph{The output PUPA is the processsing rules and formats of the data source use as the output from the process. The factory translates the HORUS scripts to generate the processing logic to output the data.}
\pagebreak
\subsection{Work Cells}
\paragraph{The remote work cells is the fundamental processing container of the rapid information factory.}
\subsubsection{Monitor Work Cell}
\paragraph{The monitor work cell consists of a persistent recursive information schema manipulator plus a remote assessment yoke for each processing work cell the spesific BSP flow requires in the rapid information factory}
\subsubsection{Processing Work Cell}
\paragraph{The processing work cell is a combination of a remote assessment yoke, an input persistent uniform protocol agreement, an autonomous node transport and an output persistent uniform protocol agreement. The remote assessment yoke communicates to the remote assessment yoke attached to the monitor work cell. The input persistent uniform protocol agreement holds the instructions to enable the work cell to import the data into the work cell. The output persistent uniform protocol agreement holds the instructions to enable the work cell to export the data from the work cell.The autonomous node transport supplies the processing power to execute the PUPA and the yoke instructions.}
\subsubsection{Measure Work Cell}
\paragraph{The measure work cell consists of an autonomous node transport that supplies the processing power and a measure agreement precision that supplies the tests to determine if the processing was successful.}
\pagebreak
\subsection{Rapid Information Factory Data Sources}
\subsubsection{Retrieve Data Sources}
\paragraph{The retrieve data sources are external data source that requires a spesial type of persistent uniform protocol agreement called a node extractor and schema transformer that supplies the data processing instructions to transform the extraernal data into HORUS compliant data structures. The additional data workspace supplies preloaded data to assist the retrieve superstep to load the data from the external data source to create the retrieve data workspace that is the main storage structure in HORUS any retrieve data loads.}
\subsubsection{Assess Data Sources}
\paragraph{The assess data sources are a read only input from the retrieve data workspace, a reference data workspace that is a read only data source for supplying reference data for the assess procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the assess data. The assess data workspace is the main storage structure for HORUS data.}
\subsubsection{Process Data Sources}
\paragraph{The process data sources are a read only input from the assess data workspace, a reference data workspace that is a read only data source for supplying reference data for the process procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the process data. The process data workspace is the main storage structure for HORUS data processed into a data vault containing hubs, links and satellites. }
\subsubsection{Data Vault}
\paragraph{The Data Vault architecture offers a unique solution to data integration in the rapid information factory. The Data Vault is a detail oriented, historical tracking and uniquely linked set of normalised tables that support one or more functional areas of the factory that stores perfeactly as data island on top of the data lake structure to process unstructured and semi-structured data into structured data.}
\paragraph{Benefits of Data Vault Modeling}
\begin{itemize}
\item Manage and enforce compliance to Sarbanes-Oxley, HIPPA, and BASIL II in factory.
\item Identify business data defects that were not visible before the processing.
\item Rapidly reduce business cycle time for implementing enhancements.
\item Merge new business units into the organisation rapidly.
\item Rapid return-on-investment and delivery of information to new star schemas in data warehouse.
\item Consolidate disparate data stores in a homogeneous data lake.
\item Effective and Efficient Master Data Management.
\item Implement and Deploy service-oriented architecture (SOA) via factory.
\item Scale to exabytes of data in the data lake.
\item SEI CMM Level 5 compliant (Repeatable, consistent, redundant architecture).
\item Full data lineage for all data from the source systems.
\end{itemize}
\subsubsection{Transform Data Sources}
\paragraph{The transform data sources are a read only input from the process data workspace, a reference data workspace that is a read only data source for supplying reference data for the transform procudures.Reference data can iclude lists of codes and description that are valid data or lookup data to enhance the quality of the data by adding extra information to the transform data. The transform data workspace is the main storage structure for HORUS data warehouse structure that supports any analytic inquiries.}
\subsubsection{Organise Data Sources}
\paragraph{The organise data sources are read only input from the Tranform data workspace, the organise data workspace to handle any organise data manipulation, the rapid information framework for datamarts, the rapid information framework for analytics and the rapid information framework for cubes that is the main storage structures for the factory.}
\subsubsection{Report Data Sources}
\paragraph{The report data sources are read only input from the rapid information framework for datamarts, the rapid information framework for analytics and the rapid information framework for cubes. The role based access contol security process enforces any role based security access to the data sources. The rapid information framework for visualistion handles the factory's visualisation requirements. The rapid information framework for exports are the export methord for the rapid information factory and formats the HORUS compliant data structures into external data formats via a persistent uniform protocol agreement.}
\pagebreak
\section{Rapid Test Framework}
\subsection{Unit testing}
\subsubsection{Static Testing}
\paragraph{YOKE Unit Testing}
\paragraph{The YOKE unit testing enables the rapid information factory to test all the YOKE structures individually.}
\subsection{Solution Testing}
\paragraph{The solution testing performance the testing of the solution.}
\paragraph{Solution Testing Plan}
\paragraph{The solution testing plan is the process description of how to test the solution as a complete factory.}
\subsubsection{Link Testing}
\paragraph{Generate Link Test Data}
\paragraph{Prepare data for each Link Test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Link Test}
\paragraph{Execute the Link Test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Link Test}
\paragraph{Execute the Link Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\subsubsection{System Testing}
\paragraph{Generate System Test Data}
\paragraph{Prepare data for each System Test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular System Test}
\paragraph{Execute the System Test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel System Test}
\paragraph{Execute the System Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\subsubsection{Performance Testing}
\paragraph{Generate Performance Test Data}
\paragraph{Prepare data for each Performance Test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Performance Test}
\paragraph{Execute the Link Performance instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Performance Test}
\paragraph{Execute the Performance Test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Solution Completion Report}
\paragraph{The solution completion report is the combined data for the solution testing.}
\subsection{Acceptance Testing}
\paragraph{The acceptance testing performance the testing of the solution.}
\paragraph{Acceptance Testing Plan}
\paragraph{The acceptance testing plan is the process description of how to test the solution for acceptance by the users.}
\subsubsection{Acceptance Testing}
\paragraph{Generate Acceptance Test Data}
\paragraph{Prepare data for each acceptance test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Acceptance Test}
\paragraph{Execute the acceptance test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Acceptance Test}
\paragraph{Execute the acceptance test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\subsubsection{Exploratory Parallel Testing}
\paragraph{Generate Exploratory Parallel Test Data}
\paragraph{Prepare data for each exploratory parallel test to match the spesific measure agreement precision instructions.}
\paragraph{Execute Singular Exploratory Parallel Test}
\paragraph{Execute the exploratory parallel test instructions by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Execute Parallel Exploratory Parallel Test}
\paragraph{Execute the exploratory parallel test instructions in parallel by combining a remote assessment yoke, an appropiate autonomous node transport and the spesific measure agreement precision.}
\paragraph{Acceptance Completion Report}
\paragraph{The acceptance completion report is the combined data for the solution testing.}
\pagebreak
\section{Process Layer - Data Vault}
\paragraph{A data vault consists of three types of data structures:}
\begin{itemize}
\item{Hub}
\paragraph{A hub is a list of unique business keys and system keys that identifies a spesific business entity.}
\item{Satellite}
\paragraph{A satellite is a set of information describing the details of a spesific business entity.}
\item{Link}
\end{itemize}
\subsection{Time-People-Object-Location-Event}
\subsubsection{Time (Hub)}
\paragraph{The time hub contains at the following fields:}
\begin{itemize}
  \item{a surrogate key, use for connection by the rest of the data structures in the data vault.}
  \item{a business key, the business key combined from date and time for the time hub.}
  \item{the record source that can be used for lineage of the date and time keys.}
  \item{the metadata fields with information for manual updates (user/time) and the extraction date.}
\end{itemize}
\subsubsection{Time Details (Satellite)}
\paragraph{The time satellite contains at the following fields:}
\begin{itemize}
\item{a surrogate key from the Time Hub}
\item{a ISO 8601 data format for the date and time}
\end{itemize}
\subsubsection{People (Hub)}
\paragraph{The people hub contains at the following fields:}
\begin{itemize}
  \item{a surrogate key, use for connection by the rest of the data structures in the data vault.}
  \item{a business key combined from people's first name, middle name, last name and date-of-birth for the people hub.}
  \item{the record source that can be used for lineage of the people keys.}
  \item{the metadata fields with information for manual updates (user/time) and the extraction date.}
\end{itemize}
\subsubsection{People Details (Satellite)}
\paragraph{??????}
\subsubsection{Object (Hub)}
\paragraph{The object hub contains at the following fields:}
\begin{itemize}
  \item{a surrogate key, use for connection by the rest of the data structures in the data vault.}
  \item{a business key combined from object type and object identifier for the object hub.}
  \item{the record source that can be used for lineage of the object keys.}
  \item{the metadata fields with information for manual updates (user/time) and the extraction date.}
\end{itemize}
\subsubsection{Object Details (Satellite)}
\paragraph{????}
\subsubsection{Location (Hub)}
\paragraph{The ISO 6709 Standard representation of geographic point location by coordinates.}
\paragraph{The location hub contains at the following fields:}
\begin{itemize}
  \item{a surrogate key, use for connection by the rest of the data structures in the data vault.}
  \item{a business key combined from latitude, longitude and altitude for geographic point locations for the location hub.}
  \item{the record source that can be used for lineage of the location keys.}
  \item{the metadata fields with information for manual updates (user/time) and the extraction date.}
\end{itemize}
\subsubsection{Location Details (Satellite)}
\paragraph{????}
\subsubsection{Event (Hub)}
\paragraph{The event hub contains at the following fields:}
\begin{itemize}
  \item{a surrogate key, use for connection by the rest of the data structures in the data vault.}
  \item{a business key combined from event type and event identifier for the event hub.}
  \item{the record source that can be used for lineage of the event keys.}
  \item{the metadata fields with information for manual updates (user/time) and the extraction date.}
\end{itemize}
\subsubsection{Event Details (Satellite)}
\paragraph{????}
\subsubsection{Time People (Link)}
\paragraph{????}
\subsubsection{Time Object (Link)}
\paragraph{????}
\subsubsection{Time Location (Link)}
\paragraph{????}
\subsubsection{Time Event (Link)}
\paragraph{????}
\subsubsection{People Object (Link)}
\paragraph{????}
\subsubsection{People Location (Link)}
\paragraph{????}
\subsubsection{People Event (Link)}
\paragraph{????}
\subsubsection{People People (Link)}
\paragraph{????}
\subsubsection{Object Location (Link)}
\paragraph{????}
\subsubsection{Object Event (Link)}
\paragraph{????}
\subsubsection{Object object (Link)}
\paragraph{????}
\subsubsection{Location Event (Link)}
\paragraph{????}
\subsubsection{Location location (Link)}
\paragraph{????}
\subsubsection{Event Event (Link)}
\paragraph{????}
\pagebreak
\section{Transform Layer - Sun Models}
\paragraph{The transform layer converts the data into business aligned knowledge structures that the business can formulate queries to answer business questions.}
\subsection{Dimensions}
\paragraph{The dimension is the information component of the business structure.}
\subsubsection{Type 0 Dimension}
\paragraph{The Type 0 method is passive as it only insert but never updates.}
\subsubsection{Type 1 Dimension}
\paragraph{The Type 1 method is active as it overwrites old with new data but keeps no track of historical data.}
\subsubsection{Type 2 Dimension}
\paragraph{The Type 2 method is active as it tracks historical data by creating multiple records for a given natural key and keeps track of period the values was valid.}
\subsubsection{Outrigger Dimension}
\paragraph{The outrigger dimension contain a reference to another dimension table to enable linking the two dimensions attributes.}
\subsubsection{Bridge Dimension}
\paragraph{The bridge dimension is mapping solution that resolves many-to-many relationship within the dimension schema by converting it to a one-to-many and a many-to-one relationship.}
\subsubsection{Mini Dimension}
\paragraph{The mini dimension creates a new structure that moves dimensional attributes that are rapidly changing into a mini–dimension to split it from non-rapid changing dimensional attributes.}
\subsection{Facts}
\paragraph{A fact table consists of the measurements, metrics for business process.}
\subsubsection{Measures}
\paragraph{Measures are facts that store measurements and metrics that enable the apllication of mathematic process on the values.}
\subsubsection{Factless}
\paragraph{Factless facts are spesial structures that only consist of keys that links to the relavent dimensions to the facts.}
\pagebreak
\section{Schedule Framework}
\subsection{Cycle Time}
\paragraph{Cycle time describes the time use to complete a specific task from start to finish.}
\paragraph{The time is calculated as:}
\begin{equation}
CycleTime = ProcessSetupTime + ProcessTime + ProcessRecycleTime + VerifySetupTime + VerifyTime + VerifyRecycleTime
\end{equation}
\subsection{Value Stream}
\paragraph{Value stream mapping is the lean-management method for analysing the current state and designing a improved state for the series of activities.}
\subsection{Value Added Procesing Time}
\paragraph{The value added processing time is the time the factory process the data to add value.}
\subsection{Non Value Added Procesing Time}
\paragraph{The non value added processing time is the time the factory wastes while process the data.}
\subsection{Production Lead Time}
\paragraph{Production Lead Time is the total time (Value Added Procesing and Non Value Added Procesing) use to execute the data through an entire value stream.}
\subsection{Schedule Backlog}
\paragraph{The Schedule Backlog is all the required processing Persistent Uniform Protocol Agreements (PUPA) prioritised, ordered list, sorted by business value and risk. It contains the tasks to accomplish the RAPTOR flow. The Schedule Backlog often contains user stories covering functional requirements, non-functional requirements for the RAPTOR flow.}
\subsubsection{INVEST RAPTOR flow}
\paragraph{\textbf{Independent}}
\paragraph{RAPTOR flow must be independent. The PUPA must ensure that it does not interfere with other PUPAs.}
\paragraph{\textbf{Negotiable}}
\paragraph{A RAPTOR flow always negotiable. It is not an explicit contract for functionality of the PUPA. The details will be co-created by the customer and programmer during development. The improvement process will ten ensure the output is delivered with maximum efficiency.}
\paragraph{\textbf{Valuable}}
\paragraph{A RAPTOR flow must be valuable to business. No non-value add PUPA will execute in the factory.}
\paragraph{\textbf{Estimable}}
\paragraph{A RAPTOR flow must be estimated. Any PUPA must have a estimate for execution.}
\paragraph{\textbf{Small}}
\paragraph{RAPTOR flow must be as small as possible. Each PUPA should only perform singular functions.}
\paragraph{\textbf{Testable}}
\paragraph{A RAPTOR flow must be testable. Each PUPA should be testable for a known input to a know output.}
\subsubsection{SMART Tasks}
\paragraph{\textbf{Specific}}
\paragraph{A RAPTOR flow task needs to be specific and tasks to add up to the full RAPTOR flow.}
\paragraph{\textbf{Measurable}}
\paragraph{The key measure is testability for successful end-to-end results.}
\paragraph{\textbf{Achievable}}
\paragraph{The task should be achievable. Each PUPA must hold a achievable function.}
\paragraph{\textbf{Relevant}}
\paragraph{Every PUPA/task should be relevant and contributing to the RAPTOR flow in the factory. Stories are divided into tasks for the achievement of RAPTOR flow.}
\paragraph{\textbf{Time-boxed}}
\paragraph{A PUPA/task should be time-boxed: limited to a specific duration.}
\subsection{\textbf{Active Process Backlog}}
\paragraph{The active process backlog consists of the committed process tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows. The backlog uses a Drum – Buffer – Rope (DBR) scheduling methodology \cite{daniel1997scheduling} that monitors the workcells for progress and then commit the next PUPA into the process.}
\subsection{\textbf{Active Process Work Cells}}
\paragraph{The active process work cells is the combination of processing structures that is currently active in the rapid information factory. The quantity is determined by the available and required parallel processing units active in the factory.}
\subsubsection{\textbf{Process Set-up Time}}
\paragraph{The process set-up time is the time it takes the factory to construct the work cell and bring it online ready for processing data.}
\subsubsection{\textbf{Process Run Time}}
\paragraph{The process run time is the time the work cell uses to process the data against the spesific input and output PUPAs' algoritmes.}
\subsubsection{\textbf{Process Reset Time}}
\paragraph{The process reset time is the time it takes the factory to reset the spesific work cell ready for the next work cell.}
\subsection{Verify Backlog}
\paragraph{The verify backlog consists of the committed process verify tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.}
\subsection{Active Verify Backlog}
\paragraph{The active verify backlog consists of the committed verify tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.The backlog uses a Drum – Buffer – Rope (DBR) scheduling methodology \cite{daniel1997scheduling} that monitors the workcells for progress and then commit the next PUPA into the process.}
\subsection{Active Verify Work Cells}
\paragraph{The active verify work cells is the combination of processing verify structures that is currently active in the rapid information factory. The quantity is determined by the available and required parallel processing units active in the factory.}
\subsubsection{Verify Set-up Time}
\paragraph{The verify set-up time is the time it takes the factory to construct the work cell and bring it online ready for processing verification data.}
\subsubsection{Verify Run Time}
\paragraph{The verify run time is the time the work cell uses to verify the data against the spesific MAP's algoritmes.}
\subsubsection{Verify Reset Time}
\paragraph{The verify reset time is the time it takes the factory to reset the spesific work cell ready for the next work cell.}
\subsection{Information Process Log}
\paragraph{The information process log consists of the completed process and verification tasks attached to the scheduled PRISM controlled end-to-end RAPTOR flows.}
\pagebreak
\section{Improvement Processes}
\subsection{The 8 Wastes}
\subsubsection{Defects.}
\paragraph{Defects are mistakes that require extra time, resources and money to reprocess.}
\paragraph{The defects are the result of:}
\begin{itemize}
\item Poor quality control of processes (ANTs, PUPAs).
\item Poor repairs on servers.
\item Poor documentation procedures in HORUS.
\item Lack of standards in HORUS.
\item Weak or missing processes (ANTs, PUPAs).
\item A misunderstanding of customer requirements.
\item Poor inventory control (ANTs, PUPAs and Data Lake).
\item Poor design of processes (ANTs, PUPAs and Data Lake).
\item Undocumented design changes (ANTs, PUPAs and Data Lake).
\end{itemize}
\subsubsection{Overproduction.}
\paragraph{Overproduction is when too many of a spesific deliverable is delivered.}
\paragraph{The factory would cause overproduction if too many ANTs is prepared for running PUPAs:}
\begin{itemize}
\item Just-in-case production, processing data without needing it.
\item Unclear customer requirements, RAPTOR PUPA's process defective data.
\item Producing to a incorrect forecast, processing is done at incorrect time.
\item Long set-up times, ANTs take to long to start-up ready for processing.
\item Attempts to avoid long set-up times, leaving ANT's running just incase they are needed.
\item Poorly applied automation, the process does not perform as required. 
\end{itemize}
\subsubsection{Waiting}
\paragraph{Actual downtime that occurs whenever processing has to stop for an unplanned/planned reason.}
\paragraph{Causes of waiting can also include:}
\begin{itemize}
\item Mismatched production rates, data arrives too early or too late.
\item Very long set-up times, ANT's take too long to be ready for processing.
\item Poor RAPTOR workflow, design to workflow for optimum throughput.
\item Insufficient staffing, when human intervention is need it has to be ready at correct time.
\item Work absences, if the data is dependent on human intervention to complete a task then absences will cause waiting.
\item Poor communications, if the yokes loses communication the process will cause waiting or lack of synchronisation.
\end{itemize}
\subsubsection{Non-Utilised Talent}
\paragraph{Non-Utilised Talent is the poor utilization of available talents, ideas, abilities and skill sets.}
\begin{itemize}
\item Lack of teamwork between staff members.
\item Lack of training in operations of the factory.
\item Poor communications between staff members and the factory.
\item Management's refusal to include employees in problem-solving.
\item Narrowly defined jobs and expectations for staff.
\item Poor management of staff.
\end{itemize}
\subsubsection{Transportation}
\paragraph{Transportation is the unnecessary moving data around within the factory.}
\begin{itemize}
\item Poor factory layout via HORUS.
\item Excessive or unnecessary handling of data via PUPAs.
\item Misaligned process flow in the factory via HORUS.
\item Poorly-designed factory PUPAs via HORUS.
\item Unnecessary steps in factory PUPA processes via HORUS.
\end{itemize}
\subsubsection{Inventory}
\paragraph{Lean are based on the practice of Just-In-Time production of data in the factory.}
\paragraph{Excess inventory is caused by:}
\begin{itemize}
\item Overproduction.
\item Poor layout.
\item Mismatched production speeds.
\item Unreliable suppliers of data.
\item Long set-up times of ANTs.
\item Misunderstood customer requirements.
\end{itemize}
\subsubsection{Motion}
\paragraph{Excess motion is to move around too much and then causes the factory slow down significantly.}
\paragraph{Causes of excessive motion include:}
\begin{itemize}
\item Poor work cell and workflow in the RAPTOR flow.
\item Poor housekeeping of ANT's recycle process.
\item Shared ANT and PUPA processing.
\item Work cell congestion
\item Isolated operations in PUPA.
\item Lack of standards in PUPA.
\item Poor process design and controls of PUPA.
\end{itemize}
\subsubsection{Extra-processing}
\paragraph{Excess Processing is any unnecessary effort expended in order to complete a task: double-handling data, seeking permission during processing, unnecessary processing steps, unnecessary data useage, re-entering data, making too many copies of data.}
\paragraph{Excess Processing arises from:}
\begin{itemize}
\item Poor process control of the RAPTOR flow via HORUS.
\item Lack of standards in PUPA.
\item Poor communication between yoke and PRISM.
\item Overdesigned equipment for ANT requirements.
\item Misunderstanding of the customer's requirements via HORUS.
\item Human error.
\item Producing to forecast not requirement by factory via HORUS.
\end{itemize}
\subsection{Plan-Do-Act-Check Improvement Process}
\subsubsection{Plan}
\paragraph{Establish the objectives and processes necessary to deliver results in accordance with the expected output (the target or goals). By establishing output expectations, the completeness and accuracy of the specifications is also a part of the targeted improvement. When possible start on a small scale to test possible effects.}
\subsubsection{Do}
\paragraph{Implement the plan, execute the process, execute the factory. Collect data for charting and analysis in the following "CHECK" and "ACT" steps.}
\subsubsection{Check}
\paragraph{Study the actual results (measured and collected in "DO" above) and compare against the expected results (targets or goals from the "PLAN") to ascertain any differences. Look for deviation in implementation from the plan and also look for the appropriateness and completeness of the plan to enable the execution, i.e., "Do". Charting data can make this much easier to see trends over several PDCA cycles and in order to convert the collected data into information. Information is what you need for the next step "ACT".}
\subsubsection{Act}
\paragraph{If the CHECK shows that the PLAN that was implemented in DO is an improvement to the prior standard (baseline), then that becomes the new standard (baseline) for how the factory should ACT going forward (new standards are enACTed). If the CHECK shows that the PLAN that was implemented in DO is not an improvement, then the existing standard (baseline) will remain in place. In either case, if the CHECK showed something different than expected (whether better or worse), then there is more learning to be done and that will suggest potential future PDCA cycles. Note that some who teach PDCA assert that the ACT involves making adjustments or corrective actions. It is a repeating process that improves the factory.}
\subsection{Define-Measure-Analyse-Improve-Control Improvement Process}
\subsubsection{Define}
\paragraph{The purpose of this step is to clearly articulate the business problem, goal, potential resources, project scope and high-level project timeline. This information is typically captured within project charter document.}
\subsubsection{Measure}
\paragraph{The purpose of this step is to objectively establish current baselines as the basis for improvement. This is a data collection step, the purpose of which is to establish process performance baselines.}
\subsubsection{Analyse}
\paragraph{The purpose of this step is to identify, validate and select root cause for elimination.}
\subsubsection{Improve}
\paragraph{The purpose of this step is to identify, test and implement a solution to the problem; in part or in whole.}
\subsubsection{Control}
\paragraph{The purpose of this step is to sustain the gains. Monitor the improvements to ensure continued and sustainable success. Create a control plan.}
\subsection{Lean Six Sigma: 5S}
\subsubsection{Sort}
\paragraph{The first step is to go through all equipment (ANTs and PUPAs) and data in the factory and determine what must be retained in the factory. Only essential PRISMs, ANTs and PUPA are allowed to remain. Archive any unwanted entities.}
\subsubsection{Set in Order}
\paragraph{“A place for everything, and everything in its place.” }
\paragraph{Work through the HORUS structures and ensure all the PRISMs, ANTs and PUPAs are in correct workflows. Use a directed acyclic graph (DAG) to ensure the workflow has a logical start and end. Simulate the process to run through without data to verify the factory before processing happens.}
\subsubsection{Shine}
\paragraph{Monitor and maintain the processes the synaptic assimilator setup in HORUS, thoroughly clean everything remaining in the factories.Ensure effective and efficient execution of PRISMs, ANTs and PUPAs in the factories.}
\subsubsection{Standardise}
\paragraph{Make factories consistent. All PRISMs and PUPAs should be identical so that any ANTs can immediately get initialise and productively execute the process if necessary.}
\subsubsection{Systematise}
\paragraph{This final step means to attach a schedule to use the 5S-ed factory flows.}
\pagebreak
\subsection{Rapid Information Factory Cluster (RIFC)}
\subsubsection{3D Torus Network Framework}
\paragraph{Torus networks are use by top-performing supercompute because it ensures that node in the cluster can directly communicate with the other nodes. A 3D Torus Network enbles a minimum hop network.}
{The network ha sthe following advantages:}
\begin{itemize}
  \item Ultra low latency message exchange
  \item Extremely high hardware message rate
  \item Low memory footprint
  \item Switchless Design - 3D Torus Direct Network
  \item High scalability
  \item Single chip solution
  \item Extremely efficient, pipelined hardware architecture
\end{itemize}
\subsubsection{MapR Data Lake}
\paragraph{The rapid information factory cluster is a bulk synchronous parallel (BSP) engine. The cluster consisting of two hunderd thousand amazon cloud nodes (d2.8xlarge with thirty six processing cores, two hunderd forty four gigabyte memory and twenty four two thousand gigabyte hard disks) to support quintillion calculations per second against quintillion of disk storage.}
\paragraph{The hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}
\subsubsection{Titan Graph Data Lake}
\paragraph{The two hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}\subsubsection{Cassandra Data Lake}
\paragraph{The five hunderd amazon graphical enhanced nodes (g2.8xlarge with four GPUs each with one thousand five hunderd CUDA cores, four gigabyte of video memory, thirty two cpus, sixty gigabyte memory and two hunderd and forty gigabyte solid state drives.)}
\pagebreak
\section{Experiments}
\subsection{Autonomous Node Transport - Build Test}
\subsubsection{Amazon Web Services ANTs}
\paragraph{A001000001 - t2.micro - Amazon Cloud instance with 1 CPUs and 1 Memory (GB)}
\paragraph{A001000002 - t2.small - Amazon Cloud instance with 1 CPUs and 2 Memory (GB)}
\paragraph{A001000003 - t2.medium - Amazon Cloud instance with 2 CPUs and 4 Memory (GB)}
\paragraph{A001000004 - t2.large - Amazon Cloud instance with 2 CPUs and 8 Memory (GB)}
\paragraph{A001000005 - m4.large - Amazon Cloud instance with 2 CPUs and EBS-only Memory (GB) with 450 mbps Throughput}
\paragraph{A001000006 - m4.xlarge - Amazon Cloud instance with 4 CPUs and EBS-only Memory (GB) with 750 mbps Throughput}
\paragraph{A001000007 - m4.2xlarge - Amazon Cloud instance with 8 CPUs and EBS-only Memory (GB) with 1000 mbps Throughput}
\paragraph{A001000008 - m4.4xlarge - Amazon Cloud instance with 16 CPUs and EBS-only Memory (GB) with 2000 mbps Throughput}
\paragraph{A001000009 - m4.10xlarge - Amazon Cloud instance with 40 CPUs and EBS-only Memory (GB) with 4000 mbps Throughput}
\paragraph{A001000010 - m3.medium - Amazon Cloud instance with 1 CPUs and 3.75 Memory (GB) with 1 x 4 GB SSD}
\paragraph{A001000011 - m3.large - Amazon Cloud instance with 2 CPUs and 7.5 Memory (GB) with 1 x 32 GB SSD}
\paragraph{A001000012 - m3.xlarge - Amazon Cloud instance with 4 CPUs and 15 Memory (GB) with 2 x 40 GB SSD}
\paragraph{A001000013 - m3.2xlarge - Amazon Cloud instance with 8 CPUs and 30 Memory (GB) with 2 x 80 GB SSD}
\paragraph{A001000014 - c4.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 500 mbps Throughput}
\paragraph{A001000015 - c4.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 750 mbps Throughput}
\paragraph{A001000016 - c4.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 1000 mbps Throughput}
\paragraph{A001000017 - c4.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2000 mbps Throughput}
\paragraph{A001000018 - c4.8xlarge - Amazon Cloud instance with 36 CPUs and 60 Memory (GB) with 4000 mbps Throughput}
\paragraph{A001000019 - c3.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 2 x 16 GB SSD}
\paragraph{A001000020 - c3.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 2 x 40 GB SSD}
\paragraph{A001000021 - c3.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 2 x 80 GB SSD}
\paragraph{A001000022 - c3.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2 x 160 GB SSD}
\paragraph{A001000023 - c3.8xlarge - Amazon Cloud instance with 32 CPUs and 60 Memory (GB) with 2 x 320 GB SSD}
\paragraph{A001000024 - r3.large - Amazon Cloud instance with 2 CPUs and 15.25 Memory (GB) with 1 x 32 GB SSD}
\paragraph{A001000025 - r3.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 80 GB SSD}
\paragraph{A001000026 - r3.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 1 x 160 GB SSD}
\paragraph{A001000027 - r3.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 1 x 320 GB SSD}
\paragraph{A001000028 - r3.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 2 x 320 GB SSD}
\paragraph{A001000029 - g2.2xlarge - Amazon Cloud instance with 1 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 8 CPUs and 15 Memory (GB) with 1 x 60 GB SSD}
\paragraph{A001000030 - g2.8xlarge - Amazon Cloud instance with 4 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 32 CPUs and 60 Memory (GB) with 2 x 120 GB SSD}
\paragraph{A001000031 - i2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 800 SSD GB SSD}
\paragraph{A001000032 - i2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 2 x 800 SSD GB SSD}
\paragraph{A001000033 - i2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 4 x 800 SSD GB SSD}
\paragraph{A001000034 - i2.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 8 x 800 SSD GB SSD}
\paragraph{A001000035 - d2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 3 x 2000 HDD GB SSD}
\paragraph{A001000036 - d2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 6 x 2000 HDD GB SSD}
\paragraph{A001000037 - d2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 12 x 2000 HDD GB SSD}
\paragraph{A001000038 - d2.8xlarge - Amazon Cloud instance with 36 CPUs and 244 Memory (GB) with 24 x 2000 HDD GB SSD}

\subsubsection{Google Cloud Platform ANTs}
\paragraph{ANT002000001 - App Engine}
\paragraph{ANT002000002 - Compute Engine}
\paragraph{ANT002000003 - Container Engine}
\paragraph{ANT002000004 - Cloud Storage}
\paragraph{ANT002000005 - Cloud Datastore}
\paragraph{ANT002000006 - Cloud SQL}
\paragraph{ANT002000007 - Cloud Big Table}
\paragraph{ANT002000008 - BigQuery}
\paragraph{ANT002000009 - Cloud Dataflow}
\paragraph{ANT002000010 - Cloud Pub/Sub}
\paragraph{ANT002000010 - Cloud Endpoints}
\paragraph{ANT002000010 - Translate API}
\paragraph{ANT002000010 - Prediction API}
\subsubsection{OpenStack ANTs}
\paragraph{ANT003000001 - ?????}
\paragraph{Redhat 2 Intel Haswell-EP processors, 24 cores, 64GB memory @2133 MHz, and seven available PCI gen3 slots}
\paragraph{ANT003000002 - ?????}
\paragraph{Ubuntu OpenStack} 
\href{http://www.ubuntu.com/download/cloud/install-ubuntu-openstack}{See Openstack}
\paragraph{ANT003000003 - ?????}
\paragraph{ANT003000004 - ?????}
\subsubsection{Physical Cluster ANTs}
\paragraph{ANT004000001 - ?????}
\paragraph{ANT004000002 - ?????}
\paragraph{ANT004000003 - ?????}
\paragraph{ANT004000004 - ?????}
\subsection{Persistent Uniform Protocol Agreements - Build Test}
\subsubsection{Amazon Web Services ANTs}
\paragraph{A001000001 - t2.micro - Amazon Cloud instance with 1 CPUs and 1 Memory (GB)}
\paragraph{A001000002 - t2.small - Amazon Cloud instance with 1 CPUs and 2 Memory (GB)}
\paragraph{A001000003 - t2.medium - Amazon Cloud instance with 2 CPUs and 4 Memory (GB)}
\paragraph{A001000004 - t2.large - Amazon Cloud instance with 2 CPUs and 8 Memory (GB)}
\paragraph{A001000005 - m4.large - Amazon Cloud instance with 2 CPUs and EBS-only Memory (GB) with 450 mbps Throughput}
\paragraph{A001000006 - m4.xlarge - Amazon Cloud instance with 4 CPUs and EBS-only Memory (GB) with 750 mbps Throughput}
\paragraph{A001000007 - m4.2xlarge - Amazon Cloud instance with 8 CPUs and EBS-only Memory (GB) with 1000 mbps Throughput}
\paragraph{A001000008 - m4.4xlarge - Amazon Cloud instance with 16 CPUs and EBS-only Memory (GB) with 2000 mbps Throughput}
\paragraph{A001000009 - m4.10xlarge - Amazon Cloud instance with 40 CPUs and EBS-only Memory (GB) with 4000 mbps Throughput}
\paragraph{A001000010 - m3.medium - Amazon Cloud instance with 1 CPUs and 3.75 Memory (GB) with 1 x 4 GB SSD}
\paragraph{A001000011 - m3.large - Amazon Cloud instance with 2 CPUs and 7.5 Memory (GB) with 1 x 32 GB SSD}
\paragraph{A001000012 - m3.xlarge - Amazon Cloud instance with 4 CPUs and 15 Memory (GB) with 2 x 40 GB SSD}
\paragraph{A001000013 - m3.2xlarge - Amazon Cloud instance with 8 CPUs and 30 Memory (GB) with 2 x 80 GB SSD}
\paragraph{A001000014 - c4.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 500 mbps Throughput}
\paragraph{A001000015 - c4.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 750 mbps Throughput}
\paragraph{A001000016 - c4.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 1000 mbps Throughput}
\paragraph{A001000017 - c4.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2000 mbps Throughput}
\paragraph{A001000018 - c4.8xlarge - Amazon Cloud instance with 36 CPUs and 60 Memory (GB) with 4000 mbps Throughput}
\paragraph{A001000019 - c3.large - Amazon Cloud instance with 2 CPUs and 3.75 Memory (GB) with 2 x 16 GB SSD}
\paragraph{A001000020 - c3.xlarge - Amazon Cloud instance with 4 CPUs and 7.5 Memory (GB) with 2 x 40 GB SSD}
\paragraph{A001000021 - c3.2xlarge - Amazon Cloud instance with 8 CPUs and 15 Memory (GB) with 2 x 80 GB SSD}
\paragraph{A001000022 - c3.4xlarge - Amazon Cloud instance with 16 CPUs and 30 Memory (GB) with 2 x 160 GB SSD}
\paragraph{A001000023 - c3.8xlarge - Amazon Cloud instance with 32 CPUs and 60 Memory (GB) with 2 x 320 GB SSD}
\paragraph{A001000024 - r3.large - Amazon Cloud instance with 2 CPUs and 15.25 Memory (GB) with 1 x 32 GB SSD}
\paragraph{A001000025 - r3.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 80 GB SSD}
\paragraph{A001000026 - r3.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 1 x 160 GB SSD}
\paragraph{A001000027 - r3.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 1 x 320 GB SSD}
\paragraph{A001000028 - r3.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 2 x 320 GB SSD}
\paragraph{A001000029 - g2.2xlarge - Amazon Cloud instance with 1 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 8 CPUs and 15 Memory (GB) with 1 x 60 GB SSD}
\paragraph{A001000030 - g2.8xlarge - Amazon Cloud instance with 4 GPUs High-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4GB of video memory, 32 CPUs and 60 Memory (GB) with 2 x 120 GB SSD}
\paragraph{A001000031 - i2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 1 x 800 SSD GB SSD}
\paragraph{A001000032 - i2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 2 x 800 SSD GB SSD}
\paragraph{A001000033 - i2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 4 x 800 SSD GB SSD}
\paragraph{A001000034 - i2.8xlarge - Amazon Cloud instance with 32 CPUs and 244 Memory (GB) with 8 x 800 SSD GB SSD}
\paragraph{A001000035 - d2.xlarge - Amazon Cloud instance with 4 CPUs and 30.5 Memory (GB) with 3 x 2000 HDD GB SSD}
\paragraph{A001000036 - d2.2xlarge - Amazon Cloud instance with 8 CPUs and 61 Memory (GB) with 6 x 2000 HDD GB SSD}
\paragraph{A001000037 - d2.4xlarge - Amazon Cloud instance with 16 CPUs and 122 Memory (GB) with 12 x 2000 HDD GB SSD}
\paragraph{A001000038 - d2.8xlarge - Amazon Cloud instance with 36 CPUs and 244 Memory (GB) with 24 x 2000 HDD GB SSD}
\subsubsection{Google Cloud Platform ANTs}
\paragraph{ANT002000001 - App Engine}
\paragraph{ANT002000002 - Compute Engine}
\paragraph{ANT002000003 - Container Engine}
\paragraph{ANT002000004 - Cloud Storage}
\paragraph{ANT002000005 - Cloud Datastore}
\paragraph{ANT002000006 - Cloud SQL}
\paragraph{ANT002000007 - Cloud Big Table}
\paragraph{ANT002000008 - BigQuery}
\paragraph{ANT002000009 - Cloud Dataflow}
\paragraph{ANT002000010 - Cloud Pub/Sub}
\paragraph{ANT002000010 - Cloud Endpoints}
\paragraph{ANT002000010 - Translate API}
\paragraph{ANT002000010 - Prediction API}
\subsubsection{OpenStack ANTs}
\paragraph{ANT003000001 - ?????}
\paragraph{ANT003000002 - ?????}
\paragraph{ANT003000003 - ?????}
\paragraph{ANT003000004 - ?????}
\subsubsection{Physical Cluster ANTs}
\paragraph{ANT004000001 - ?????}
\paragraph{ANT004000002 - ?????}
\paragraph{ANT004000003 - ?????}
\paragraph{ANT004000004 - ?????}
\subsection{Persistent Uniform Protocol Agreement - Process (Fundamental)}
\subsubsection{PUPA001000001 - Sort 1,000 keys assending}
\subsubsection{PUPA001000002 - Sort 1,000,000 keys assending}
\subsubsection{PUPA001000003 - Sort 1,000,000,000 keys assending}
\subsubsection{PUPA001000004 - Sort 1,000 keys across 10 nodes assending}
\subsubsection{PUPA001000005 - Sort 1,000,000 keys across 10 nodes assending}
\subsubsection{PUPA001000006 - Sort 1,000,000,000 keys 10 nodes assending}
\subsection{Persistent Uniform Protocol Agreement - Process (Advance)}
\paragraph{ The factory should be able to handle the 13 Dwalves of High-Performance Processing \cite{asanovic2006landscape}}
\subsubsection{PUPA002000001 - Dense Linear Algebra}
\paragraph{The data structure is classic vector and matrix operations that are divided into Level 1 (vector/vector), Level 2 (matrix/vector), and Level 3 (matrix/matrix) operations. Data is normally a contiguous array and computations on elements, rows, columns, or matrix blocks to produce a output.}
\paragraph{General dense (all entries nonzero)}
\paragraph{Banded (zero below/above some diagonal)}
\paragraph{Symmetric/Hermitian}
\subsubsection{PUPA002000002 - Sparse Linear Algebra}
\paragraph{Sparse matrix algorithms are use when input matrices contain a large number of zero entries. Gather-scatter is a type of memory addressing that often arises when addressing vectors in sparse linear algebra operations.}
\subsubsection{PUPA002000003 - Spectral Methods}
\paragraph{Spectral methods are a class of techniques applying mathematics and scientific computing using Fast Fourier Transform.}
\paragraph{Data is operated in the spectral domain, often transformed from either a temporal or spatial domain.}
\subsubsection{PUPA002000004 - N-Body Methods}
\paragraph{A n-body problem is the problem of predicting the individual motions of a group of celestial objects interacting with each other gravitationally.}
\paragraph{Data is arranged in a regular multidimensional grid (most commonly 2D or 3D).}
\subsubsection{PUPA002000005 - Structured Grids}
\paragraph{A structured Grid is n-dimensional Euclidean space by congruent cells. Example: a 3-dimensional grid is of format m(x,y,z).}
\subsubsection{PUPA002000006 - Unstructured Grids}
\paragraph{A unstructured grid is a grid consisting of vertices and edges that describes a system of interconnected entities.}
\paragraph{An irregular mesh or grid, with each grid element being updated from neighboring grid elements.}
\subsubsection{PUPA002000007 - Monte Carlo Simulations}
\paragraph{A problem solving technique used to approximate the probability of outcomes by executing multiple trials, called simulations, using randomised variables.}
\subsubsection{PUPA002000008 - Combinational Logic}
\paragraph{A type of digital logic used by Boolean circuits, where the output is a pure function of the supplied input only.}
\paragraph{Combinational logic computations are performing simple operations on very large amounts of data.}
\subsubsection{PUPA002000009 - Graph Traversal}
\paragraph{Graph traversal is the problem of visiting all the nodes in a graph in a particular order.}
\paragraph{Graph Traversal applications traverse a number of objects and examine characteristics of those objects to match spesific patterns.}
\subsubsection{PUPA002000010 - Dynamic Programming}
\paragraph{Dynamic programming is a extremely powerful algorithmic paradigm solving a problem by identifying a collection of subproblems and solving them one by one, smallest first, using the results to small problems to solve larger ones, until the whole set of them is solved successfully.}
\subsubsection{PUPA002000011 - Backtrack and Branch-and-Bound}
\paragraph{Branch and bound algorithms are effective for solving various search and global optimization problems. The goal in such problems is to search a very large space to find a globally optimal solution.}
\subsubsection{PUPA002000012 - Graphical Models}
\paragraph{A graphical model or probabilistic graphical model (PGM) is a probabilistic model for a graph expressing the conditional dependence structure between random variables.}
\paragraph{Graphical models include Bayesian networks (also known as belief networks, probabilistic networks, causal network, and knowledge maps). Hidden Markov models and neural networks are also graphical models.}
\subsubsection{PUPA002000013 - Finite State Machine}
\paragraph{A finite-state machine (FSM) is a mathematical model of computation used to design computer programs and sequential logic circuits. It is modelled as an abstract machine that can be in one of a finite number of states.}
\subsection{Persistent Uniform Protocol Agreement - Verify}
\subsubsection{?????????????????????????}
\paragraph{??????????}
\subsubsection{?????????????????????????}
\paragraph{??????????}
\subsubsection{?????????????????????????}
\paragraph{??????????}
\subsection{Apache Hama}
\paragraph{Apache Hama is a distributed computing framework based on Bulk Synchronous Parallel computing techniques for massive scientific computations e.g., matrix, graph and network algorithms.\cite{seo2010hama}}
\subsection{Apache Giraph}
\paragraph{Giraph is an iterative graph processing system built for high scalability.\cite{ching2013scaling}}
\subsection{Weaver}
\paragraph{A fast and scalable graph store designed specifically for dynamically-changing graphs.\cite{wu2012kernel}}
\subsection{Medusa}
\paragraph{A framework for graph processing using Graphics Processing Units (GPUs) on both shared memory and distributed clusters.\cite{zhong2014medusa}}
\subsection{Titan}
\paragraph{Titan is a scalable graph database optimized for storing and querying graphs containing hundreds of billions of vertices and edges distributed across a multi-machine cluster. Titan is a transactional database that can support thousands of concurrent users executing complex graph traversals in real time. It supports ACID and eventual consistency and various big data storage back-ends.\cite{chang1997titan}}
\pagebreak
\section{Results}
\paragraph{?????????????????????????}
\paragraph{?????????????????????????}
\paragraph{?????????????????????????}
\paragraph{?????????????????????????}
\pagebreak
\bibliography{andreasfrancoisvermeulen}
\bibliographystyle{acm}
\end{document}
